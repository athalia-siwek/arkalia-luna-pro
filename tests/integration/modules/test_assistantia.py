#!/usr/bin/env python3
"""
ğŸ§ª Test AssistantIA - Script de validation v2.8.0
Teste le bon fonctionnement d'AssistantIA avec Ollama
"""

import asyncio
import json
import time
from pathlib import Path
from typing import Any

import requests

from core.ark_logger import ark_logger


class AssistantIATester:
    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url
        self.test_results = []

    def log_test(self, test_name: str, success: bool, details: str = ""):
        """Enregistre un rÃ©sultat de test"""
        result = {
            "test": test_name,
            "success": success,
            "details": details,
            "timestamp": time.time(),
        }
        self.test_results.append(result)

        status = "âœ…" if success else "âŒ"
        ark_logger.info(f"{status} {test_name}: {details}", extra={"module": "scripts"})

    def test_health_endpoint(self) -> bool:
        """Teste l'endpoint de santÃ©"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=10)
            if response.status_code == 200:
                data = response.json()
                self.log_test("Health Check", True, f"Status: {data.get('status')}")
                return True
            else:
                self.log_test("Health Check", False, f"Status code: {response.status_code}")
                return False
        except Exception as e:
            self.log_test("Health Check", False, f"Erreur: {str(e)}")
            return False

    def test_models_endpoint(self) -> bool:
        """Teste l'endpoint des modÃ¨les disponibles"""
        try:
            response = requests.get(f"{self.base_url}/models", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                model_names = [m.get("name", "") for m in models]
                self.log_test("Models Endpoint", True, f"ModÃ¨les: {', '.join(model_names)}")
                return True
            else:
                self.log_test("Models Endpoint", False, f"Status code: {response.status_code}")
                return False
        except Exception as e:
            self.log_test("Models Endpoint", False, f"Erreur: {str(e)}")
            return False

    def test_chat_endpoint(self, message: str = "Bonjour, comment allez-vous ?") -> bool:
        """Teste l'endpoint de chat"""
        try:
            payload = {
                "message": message,
                "model": "mistral:latest",
                "temperature": 0.7,
                "include_context": True,
            }

            response = requests.post(f"{self.base_url}/api/v1/chat", json=payload, timeout=30)

            if response.status_code == 200:
                data = response.json()
                response_text = data.get("response", "")
                processing_time = data.get("processing_time", 0)
                context_quality = data.get("context_quality", 0)

                self.log_test(
                    "Chat Endpoint",
                    True,
                    f"RÃ©ponse: {response_text[:50]}... | Temps: {processing_time:.2f}s | QualitÃ© contexte: {context_quality}",
                )
                return True
            else:
                self.log_test("Chat Endpoint", False, f"Status code: {response.status_code}")
                return False
        except Exception as e:
            self.log_test("Chat Endpoint", False, f"Erreur: {str(e)}")
            return False

    def test_ollama_connection(self) -> bool:
        """Teste la connexion Ã  Ollama"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                model_names = [m.get("name", "") for m in models]
                self.log_test(
                    "Ollama Connection", True, f"ModÃ¨les disponibles: {', '.join(model_names)}"
                )
                return True
            else:
                self.log_test("Ollama Connection", False, f"Status code: {response.status_code}")
                return False
        except Exception as e:
            self.log_test("Ollama Connection", False, f"Erreur: {str(e)}")
            return False

    def test_arkalia_context(self) -> bool:
        """Teste la rÃ©cupÃ©ration du contexte Arkalia"""
        try:
            # VÃ©rifier les fichiers d'Ã©tat
            state_files = [
                "state/zeroia_dashboard.json",
                "state/reflexia_state.toml",
                "state/sandozia",
                "state/cognitive_reactor_state.toml",
            ]

            available_contexts = []
            for file_path in state_files:
                if Path(file_path).exists():
                    available_contexts.append(Path(file_path).name)

            if available_contexts:
                self.log_test(
                    "Arkalia Context", True, f"Contexte disponible: {', '.join(available_contexts)}"
                )
                return True
            else:
                self.log_test("Arkalia Context", False, "Aucun contexte disponible")
                return False
        except Exception as e:
            self.log_test("Arkalia Context", False, f"Erreur: {str(e)}")
            return False

    def test_metrics_endpoint(self) -> bool:
        """Teste l'endpoint des mÃ©triques Prometheus"""
        try:
            response = requests.get(f"{self.base_url}/metrics", timeout=10)
            if response.status_code == 200:
                content = response.text
                if "assistantia_prompts_total" in content:
                    self.log_test("Metrics Endpoint", True, "MÃ©triques Prometheus disponibles")
                    return True
                else:
                    self.log_test("Metrics Endpoint", False, "MÃ©triques Prometheus manquantes")
                    return False
            else:
                self.log_test("Metrics Endpoint", False, f"Status code: {response.status_code}")
                return False
        except Exception as e:
            self.log_test("Metrics Endpoint", False, f"Erreur: {str(e)}")
            return False

    def run_all_tests(self) -> dict[str, Any]:
        """ExÃ©cute tous les tests"""
        ark_logger.info("ğŸ§ª DÃ©marrage des tests AssistantIA...", extra={"module": "scripts"})
        ark_logger.info("=" * 50, extra={"module": "scripts"})

        tests = [
            ("Ollama Connection", self.test_ollama_connection),
            ("Health Endpoint", self.test_health_endpoint),
            ("Models Endpoint", self.test_models_endpoint),
            ("Arkalia Context", self.test_arkalia_context),
            ("Chat Endpoint", self.test_chat_endpoint),
            ("Metrics Endpoint", self.test_metrics_endpoint),
        ]

        passed = 0
        total = len(tests)

        for _test_name, test_func in tests:
            if test_func():
                passed += 1

        ark_logger.info("=" * 50, extra={"module": "scripts"})
        ark_logger.info(
            f"ğŸ“Š RÃ©sultats: {passed}/{total} tests rÃ©ussis", extra={"module": "scripts"}
        )

        # Test de chat avancÃ©
        ark_logger.info("\nğŸ¤– Test de conversation avancÃ©e...", extra={"module": "scripts"})
        advanced_tests = [
            "Explique-moi le rÃ´le de ZeroIA dans Arkalia-LUNA",
            "Quelle est la diffÃ©rence entre Reflexia et Sandozia ?",
            "Comment fonctionne le systÃ¨me de monitoring ?",
        ]

        for question in advanced_tests:
            ark_logger.info(f"\nâ“ Question: {question}", extra={"module": "scripts"})
            self.test_chat_endpoint(question)
            time.sleep(2)  # Pause entre les questions

        return {
            "total_tests": total,
            "passed_tests": passed,
            "failed_tests": total - passed,
            "success_rate": (passed / total) * 100 if total > 0 else 0,
            "test_results": self.test_results,
        }

    def save_results(self, filename: str = "logs/assistantia_test_results.json"):
        """Sauvegarde les rÃ©sultats des tests"""
        try:
            Path(filename).parent.mkdir(exist_ok=True)
            with open(filename, "w") as f:
                json.dump(self.test_results, f, indent=2)
            ark_logger.info(
                f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans {filename}", extra={"module": "scripts"}
            )
        except Exception as e:
            ark_logger.info(f"âŒ Erreur sauvegarde: {e}", extra={"module": "scripts"})


def main():
    """Fonction principale"""
    tester = AssistantIATester()
    results = tester.run_all_tests()
    tester.save_results()

    ark_logger.info("\nğŸ¯ RÃ©sumÃ© final:", extra={"module": "scripts"})
    ark_logger.info(
        f"   Tests rÃ©ussis: {results['passed_tests']}/{results['total_tests']}",
        extra={"module": "scripts"},
    )
    ark_logger.info(
        f"   Taux de succÃ¨s: {results['success_rate']:.1f}%", extra={"module": "scripts"}
    )

    if results["success_rate"] >= 80:
        ark_logger.info("ğŸ‰ AssistantIA fonctionne correctement !", extra={"module": "scripts"})
        return 0
    else:
        ark_logger.info("âš ï¸  AssistantIA nÃ©cessite des corrections.", extra={"module": "scripts"})
        return 1


if __name__ == "__main__":
    exit(main())
