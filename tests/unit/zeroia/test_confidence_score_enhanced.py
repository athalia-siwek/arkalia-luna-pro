"""üß™ Tests √©tendus pour le module confidence_score de ZeroIA"""

import json
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from unittest.mock import mock_open, patch

import pytest
import toml

from modules.zeroia.confidence_score import ConfidenceScorer


class TestConfidenceScoreEnhanced:
    """Tests √©tendus pour le syst√®me de scoring de confiance"""

    def setup_method(self):
        """Configuration avant chaque test"""
        self.temp_dir = tempfile.mkdtemp()
        self.state_file = Path(self.temp_dir) / "confidence_memory.toml"
        self.scorer = ConfidenceScorer(str(self.state_file))

    def test_confidence_scorer_initialization(self):
        """Test de l'initialisation du ConfidenceScorer"""
        assert self.scorer.confidence_threshold == 0.7
        assert isinstance(self.scorer.pattern_weights, dict)
        assert "consistency" in self.scorer.pattern_weights
        assert isinstance(self.scorer.memory, dict)

    def test_load_config_success(self):
        """Test du chargement r√©ussi de la configuration"""
        mock_config = {"threshold": 0.8, "decay_rate": 0.15}
        with patch("builtins.open", mock_open(read_data=toml.dumps(mock_config))):
            result = self.scorer.load_config()
            assert result == mock_config

    def test_load_config_file_not_found(self):
        """Test du chargement avec fichier introuvable"""
        with patch("builtins.open", side_effect=FileNotFoundError):
            result = self.scorer.load_config()
            assert result == {"threshold": 0.7, "decay_rate": 0.1}

    def test_load_config_invalid_data(self):
        """Test du chargement avec donn√©es invalides"""
        with patch("builtins.open", side_effect=Exception("Invalid data")):
            result = self.scorer.load_config()
            assert result == {"threshold": 0.7, "decay_rate": 0.1}

    def test_load_memory_new_file(self):
        """Test du chargement de m√©moire pour un nouveau fichier"""
        # Supprimer le fichier s'il existe
        if self.state_file.exists():
            self.state_file.unlink()

        memory = self.scorer._load_memory()
        assert "decision_patterns" in memory
        assert "successful_contexts" in memory
        assert "error_contexts" in memory
        assert "performance_metrics" in memory
        assert "learning_weights" in memory
        assert "last_update" in memory

    def test_load_memory_existing_file(self):
        """Test du chargement de m√©moire depuis un fichier existant"""
        mock_memory = {
            "decision_patterns": {"test": "monitor"},
            "successful_contexts": [],
            "error_contexts": [],
            "performance_metrics": {},
            "learning_weights": self.scorer.pattern_weights.copy(),
            "last_update": datetime.now().isoformat(),
        }

        with patch("builtins.open", mock_open(read_data=toml.dumps(mock_memory))):
            with patch("pathlib.Path.exists", return_value=True):
                memory = self.scorer._load_memory()
                assert memory["decision_patterns"] == {"test": "monitor"}

    def test_save_memory_success(self):
        """Test de la sauvegarde r√©ussie de la m√©moire"""
        with patch("builtins.open", mock_open()) as mock_file:
            self.scorer._save_memory()
            mock_file.assert_called_once()

    def test_calculate_confidence_basic(self):
        """Test du calcul de confiance de base"""
        decision = "monitor"
        context = {"cpu": 50.0, "ram": 60.0}

        score, explanation = self.scorer.calculate_confidence(decision, context)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert isinstance(explanation, dict)
        assert "final_score" in explanation
        assert "decision" in explanation
        assert "timestamp" in explanation

    def test_calculate_confidence_with_system_metrics(self):
        """Test du calcul de confiance avec m√©triques syst√®me"""
        decision = "reduce_load"
        context = {"cpu": 80.0, "ram": 70.0}
        system_metrics = {"cpu": 85.0, "ram": 75.0, "response_time_ms": 150}

        score, explanation = self.scorer.calculate_confidence(decision, context, system_metrics)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert explanation["decision"] == decision

    def test_score_consistency_no_history(self):
        """Test du score de coh√©rence sans historique"""
        decision = "monitor"
        context = {"cpu": 50.0}

        score, explanation = self.scorer._score_consistency(decision, context)

        assert score == 0.5
        assert "Pas d'historique" in explanation

    def test_score_consistency_with_history(self):
        """Test du score de coh√©rence avec historique"""
        # Ajouter des patterns dans la m√©moire
        context1 = {"cpu": 50.0, "ram": 60.0}
        context2 = {"cpu": 55.0, "ram": 65.0}

        self.scorer.memory["decision_patterns"] = {
            json.dumps(context1): "monitor",
            json.dumps(context2): "monitor",
        }

        decision = "monitor"
        context = {"cpu": 52.0, "ram": 62.0}  # Similaire aux contextes existants

        score, explanation = self.scorer._score_consistency(decision, context)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0

    def test_score_system_health(self):
        """Test du score de sant√© syst√®me"""
        metrics = {"cpu": 50.0, "ram": 60.0}

        score, explanation = self.scorer._score_system_health(metrics)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert isinstance(explanation, str)

    def test_score_response_time(self):
        """Test du score de temps de r√©ponse"""
        metrics = {"response_time_ms": 100}

        score, explanation = self.scorer._score_response_time(metrics)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert isinstance(explanation, str)

    def test_score_resource_efficiency(self):
        """Test du score d'efficacit√© des ressources"""
        metrics = {"cpu": 50.0, "ram": 60.0}

        score, explanation = self.scorer._score_resource_efficiency(metrics)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert isinstance(explanation, str)

    def test_score_context_relevance(self):
        """Test du score de pertinence contextuelle"""
        decision = "monitor"
        context = {"cpu": 50.0, "ram": 60.0}

        score, explanation = self.scorer._score_context_relevance(decision, context)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert isinstance(explanation, str)

    def test_score_error_rate(self):
        """Test du score bas√© sur le taux d'erreur"""
        decision = "monitor"

        score, explanation = self.scorer._score_error_rate(decision)

        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0
        assert isinstance(explanation, str)

    def test_calculate_context_similarity(self):
        """Test du calcul de similarit√© entre contextes"""
        ctx1 = {"cpu": 50.0, "ram": 60.0}
        ctx2 = {"cpu": 55.0, "ram": 65.0}

        similarity = self.scorer._calculate_context_similarity(ctx1, ctx2)

        assert isinstance(similarity, float)
        assert 0.0 <= similarity <= 1.0

    def test_categorize_confidence(self):
        """Test de la cat√©gorisation de la confiance"""
        assert self.scorer._categorize_confidence(0.9) == "Tr√®s √©lev√©e"
        assert self.scorer._categorize_confidence(0.7) == "√âlev√©e"
        assert self.scorer._categorize_confidence(0.5) == "Moyenne"
        assert self.scorer._categorize_confidence(0.3) == "Faible"
        assert self.scorer._categorize_confidence(0.1) == "Tr√®s faible"

    def test_generate_recommendations(self):
        """Test de la g√©n√©ration de recommandations"""
        scores = {
            "consistency": 0.8,
            "system_health": 0.6,
            "response_time": 0.7,
            "resource_efficiency": 0.5,
            "context_relevance": 0.9,
            "error_rate": 0.8,
        }
        final_score = 0.7

        recommendations = self.scorer._generate_recommendations(scores, final_score)

        assert isinstance(recommendations, list)
        assert all(isinstance(rec, str) for rec in recommendations)

    def test_get_memory_summary(self):
        """Test de l'obtention du r√©sum√© de m√©moire"""
        # Ajouter des donn√©es de test dans la m√©moire
        self.scorer.memory["decision_patterns"] = {"test": "monitor"}
        self.scorer.memory["successful_contexts"] = [{"test": "success"}]
        self.scorer.memory["performance_metrics"] = {"avg_confidence": 0.8}

        summary = self.scorer.get_memory_summary()

        assert isinstance(summary, dict)
        # V√©rifier que le r√©sum√© contient les bonnes cl√©s selon l'impl√©mentation
        assert "status" in summary or "total_decisions" in summary

    def test_update_confidence(self):
        """Test de la mise √† jour de la confiance"""
        decision_id = "test_decision_123"
        new_confidence = 0.8

        # Ne doit pas lever d'exception
        self.scorer.update_confidence(decision_id, new_confidence)

    def test_get_average_confidence(self):
        """Test de l'obtention de la confiance moyenne"""
        avg_confidence = self.scorer.get_average_confidence()

        assert isinstance(avg_confidence, float)
        assert 0.0 <= avg_confidence <= 1.0

    def test_decay_confidence(self):
        """Test de la d√©croissance de la confiance"""
        # Ne doit pas lever d'exception
        self.scorer.decay_confidence(days=7)

    def test_save_confidence_data(self):
        """Test de la sauvegarde des donn√©es de confiance"""
        # Ne doit pas lever d'exception
        self.scorer.save_confidence_data()

    def test_load_confidence_data(self):
        """Test du chargement des donn√©es de confiance"""
        # Ne doit pas lever d'exception
        self.scorer.load_confidence_data()

    def test_calculate_confidence_edge_cases(self):
        """Test du calcul de confiance avec cas limites"""
        # Test avec contexte vide
        score, explanation = self.scorer.calculate_confidence("monitor", {})
        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0

        # Test avec m√©triques syst√®me vides
        score, explanation = self.scorer.calculate_confidence("monitor", {}, {})
        assert isinstance(score, float)
        assert 0.0 <= score <= 1.0

    def test_context_similarity_edge_cases(self):
        """Test de la similarit√© de contexte avec cas limites"""
        # Test avec contextes vides
        similarity = self.scorer._calculate_context_similarity({}, {})
        assert isinstance(similarity, float)

        # Test avec contextes diff√©rents
        ctx1 = {"cpu": 50.0}
        ctx2 = {"ram": 60.0}
        similarity = self.scorer._calculate_context_similarity(ctx1, ctx2)
        assert isinstance(similarity, float)
