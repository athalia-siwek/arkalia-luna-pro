# ğŸ¤– AssistantIA â€” Module Cognitif IntÃ©grÃ©

Le module `assistantia/` est lâ€™interface dâ€™assistance IA locale dâ€™Arkalia-LUNA. Il agit comme **guide conversationnel**, interface cognitive et **rÃ©pondant intelligent** aux requÃªtes utilisateurs.

---

## ğŸ§  RÃ´le du module

- Dialogue IA avec lâ€™utilisateur
- RÃ©ponses contextuelles personnalisÃ©es
- Interface Ã©volutive vers lâ€™IA autonome embarquÃ©e
- Support aux modules (Helloria, Reflexiaâ€¦)

---

## ğŸš€ Lancement manuel

```bash
uvicorn modules.assistantia.core:app --port 8001

ğŸ“ Port configurable dans docker-compose.yml ou config/.

â¸»

ğŸ” Endpoints disponibles

MÃ©thode
URL
Description
POST
/chat
Envoie un message Ã  lâ€™IA locale
GET
/status
Ã‰tat du module assistantia

ğŸ§ª Tests associÃ©s

Fichiers :
	â€¢	test_assistantia.py (unitaires)
	â€¢	test_assistantia_integration.py (intÃ©gration)

âœ… Couverture : 81 % â€” avec plan dâ€™extension vers les cas dâ€™erreur et logs dÃ©taillÃ©s.

â¸»

ğŸŒ ConnectivitÃ© modulaire

Le module est connectÃ© Ã  :
	â€¢	helloria/ (API externe)
	â€¢	reflexia/ (logs et surveillance IA)
	â€¢	nyxalia/ (interprÃ©tation mobile)

ğŸ’¡ Il est prÃªt pour une extension vers Ollama, Langchain, ou des modÃ¨les hybrides.

â¸»

ğŸ¯ Objectif futur : une IA embarquÃ©e rÃ©flexive, contextuelle, auto-ajustable.

# ğŸ§  AssistantIA â€” Utilisation et IntÃ©gration LLM

L'AssistantIA est conÃ§u pour offrir une interaction fluide et intelligente avec les utilisateurs, en intÃ©grant des modÃ¨les de langage de pointe (LLM) pour comprendre et rÃ©pondre aux requÃªtes de maniÃ¨re contextuelle.

## ğŸš€ FonctionnalitÃ©s Principales

- **RÃ©ponses Contextuelles** : GrÃ¢ce Ã  l'intÃ©gration de modÃ¨les LLM comme Mistral et Llama2, l'AssistantIA peut fournir des rÃ©ponses prÃ©cises et adaptÃ©es au contexte de la conversation.
- **Personnalisation** : L'AssistantIA s'adapte aux prÃ©fÃ©rences de l'utilisateur, offrant une expÃ©rience personnalisÃ©e.
- **IntÃ©gration Facile** : Peut Ãªtre intÃ©grÃ© dans diverses applications via des API REST, facilitant l'interaction avec d'autres systÃ¨mes.

## ğŸŒ Exemple d'Utilisation

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quelle est la philosophie d'Arkalia ?"}'
```

## ğŸ§  ModÃ¨les LLM IntÃ©grÃ©s

L'AssistantIA utilise des modÃ¨les LLM locaux pour garantir la confidentialitÃ© et l'efficacitÃ©. Les modÃ¨les sont stockÃ©s localement et peuvent Ãªtre mis Ã  jour ou remplacÃ©s selon les besoins.

## ğŸ“Š Structure JSON Entrante/Sortante

### RequÃªte

```json
{
  "message": "Bonjour Arkalia",
  "mode": "empathique",
  "lang": "fr",
  "user_id": "12345"
}
```

### RÃ©ponse

```json
{
  "rÃ©ponse": "Bonjour ! Je suis AssistantIA, prÃªt Ã  vous aider."
}
```

## âš™ï¸ ParamÃ¨tres Optionnels

- **mode** : DÃ©finit le mode de raisonnement de l'IA (ex: neutre, empathique).
- **lang** : Langue de rÃ©ponse attendue (ex: fr, en).
- **user_id** : Identifiant utilisateur pour personnalisation.

## ğŸ“ˆ SchÃ©ma d'Interaction

```mermaid
sequenceDiagram
    participant U as Utilisateur
    participant A as AssistantIA
    participant O as Ollama
    U->>A: POST /chat (message)
    A->>O: Query modÃ¨le
    O-->>A: RÃ©ponse IA
    A-->>U: JSON { "rÃ©ponse": "..." }
```

---

ğŸ§  *L'AssistantIA est votre partenaire intelligent pour une interaction IA enrichissante et sÃ©curisÃ©e.*

Pour des considÃ©rations de sÃ©curitÃ©, veuillez consulter [SÃ©curitÃ©](security.md).