# ğŸ§  Ollama â€” ModÃ¨les LLM Locaux

> IntÃ©gration de modÃ¨les LLM **100 % locaux**, sans cloud, via lâ€™outil **[Ollama](https://ollama.com/)**.

---

## ğŸ“ Chemin de Stockage

Les modÃ¨les sont stockÃ©s ici :

```bash
/Volumes/T7/devstation/ollama_data/models
```

## ğŸ“¦ ModÃ¨les InstallÃ©s

| Nom       | Taille approx. | Utilisation principale                      |
|-----------|----------------|---------------------------------------------|
| mistral   | ~4.1 Go        | GÃ©nÃ©ration IA contextuelle rapide           |
| llama2    | ~3.8 Go        | ModÃ¨le polyvalent, bon en gÃ©nÃ©raliste       |
| tinyllama | ~637 Mo        | LLM ultra-lÃ©ger pour tests rapides          |

ğŸ’¡ Dâ€™autres modÃ¨les peuvent Ãªtre installÃ©s Ã  la volÃ©e avec :

```bash
ollama pull <nom_du_modÃ¨le>
```

## âš™ï¸ Configuration de lâ€™Environnement

Ajoutez dans votre fichier `~/.zshrc` :

```bash
export OLLAMA_MODELS=/Volumes/T7/devstation/ollama_data/models
```

Ensuite, rechargez le shell :

```bash
source ~/.zshrc
```

## ğŸ”§ Exemple dâ€™appel via API (Ollama Serveur Local)

```bash
curl http://localhost:11434/api/generate \
  -d '{
        "model": "mistral",
        "prompt": "Bonjour, peux-tu mâ€™aider Ã  automatiser une tÃ¢che ?"
      }'
```

## ğŸ” IntÃ©gration dans Arkalia

- Le connecteur `ollama_connector.py` est disponible dans `utils/`
- Le module AssistantIA utilise `query_ollama()` pour gÃ©nÃ©rer des rÃ©ponses locales.
- Les logs sont capturÃ©s dans `logs/assistantia/`.

â¸»

ğŸ§  Arkalia-LUNA exÃ©cute ses IA localement, pour prÃ©server la confidentialitÃ© et maximiser lâ€™efficacitÃ© cognitive.
