# ğŸ§  Utilisation dâ€™Arkalia-LUNA

Ce guide prÃ©sente **comment interagir avec le systÃ¨me IA Arkalia-LUNA** via les endpoints FastAPI exposÃ©s localement. Il s'adresse aux dÃ©veloppeurs, testeurs ou utilisateurs externes souhaitant dialoguer avec l'IA, surveiller son Ã©tat ou intÃ©grer ses fonctions.

---

## ğŸš€ Lancement rapide du systÃ¨me

### ğŸ“¦ En local (Uvicorn)

```bash
uvicorn modules.helloria.core:app --reload
```

ğŸ³ En mode Docker

```bash
docker-compose up --build
```

ğŸ§  Via alias terminal

- `ark-run`         # Lancement IA local
- `ark-docker`      # Build & run docker
- `ark-test`        # Tests unitaires + couverture

ğŸŒ Endpoints API principaux

ğŸ” GET /status

Retourne lâ€™Ã©tat du systÃ¨me Arkalia.

```bash
curl -X GET http://localhost:8000/status
```

ğŸ“¥ RÃ©ponse type :

```json
{
  "status": "online",
  "modules": ["Reflexia", "Nyxalia", "AssistantIA"]
}
```

ğŸ—£ï¸ POST /chat

Permet dâ€™envoyer une requÃªte Ã  lâ€™IA locale via AssistantIA (Ollama).

```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quelle est la philosophie dâ€™Arkalia ?"}'
```

ğŸ“¥ RÃ©ponse type :

```json
{
  "response": "Tu as dit : Quelle est la philosophie dâ€™Arkalia ?"
}
```

â„¹ï¸ La rÃ©ponse dÃ©pend du modÃ¨le LLM actif (Ollama : mistral, llama2, etc.)

â¸»

ğŸ“‹ ParamÃ¨tres avancÃ©s (Ã  venir)

Le corps de requÃªte /chat supportera bientÃ´t :

| Champ    | Type   | Description                        |
|----------|--------|------------------------------------|
| message  | string | Prompt envoyÃ© Ã  lâ€™IA               |
| user_id  | string | Identifiant utilisateur (personnalisation) |
| lang     | string | Langue de rÃ©ponse attendue (fr, en, etc.) |
| mode     | string | Mode de raisonnement (neutre, empathiqueâ€¦)|

ğŸ§  Diagramme dâ€™appel API (Mermaid)

```mermaid
sequenceDiagram
  participant U as Utilisateur
  participant API as FastAPI (Helloria)
  participant IA as AssistantIA (Ollama)

  U->>API: POST /chat { message }
  API->>IA: RequÃªte LLM
  IA-->>API: RÃ©ponse texte
  API-->>U: RÃ©ponse JSON
```

ğŸ“ Notes
- Le port par dÃ©faut est 8000 (modifiable dans docker-compose.yml)
- Lâ€™endpoint /chat est exposÃ© par le module assistantia
- Toute interaction est loggÃ©e dans logs/ (si activÃ©)

â¸»

ğŸ§­ Prochaines Ã©volutions
- Authentification API (token, user_id)
- Historique conversationnel stockÃ©
- Mode debug IA interactif (debug_mode = true)

â¸»

ğŸ‘©â€ğŸ’» Maintenu par Athalia ğŸŒ™ â€” github.com/arkalia-luna-system
