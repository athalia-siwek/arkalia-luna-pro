#!/bin/bash
set -e

# ðŸŒ• ARKALIA-LUNA - SCRIPT DE TEST DE PERFORMANCE
# Version : 2.8.0 - Post Audit Tests
# Date : 4 juillet 2025

# Configuration
PERF_DIR="tests/performance"
REPORTS_DIR="tests/reports/performance"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE="$REPORTS_DIR/performance_$TIMESTAMP.log"

# Couleurs
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

print_header() {
    echo -e "${PURPLE}ðŸŒ• ARKALIA-LUNA - TESTS DE PERFORMANCE${NC}"
    echo -e "${PURPLE}=====================================${NC}"
    echo ""
}

print_status() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}" | tee -a "$LOG_FILE"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}" | tee -a "$LOG_FILE"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}" | tee -a "$LOG_FILE"
}

print_section() {
    echo -e "${CYAN}ðŸ“‹ $1${NC}" | tee -a "$LOG_FILE"
    echo -e "${CYAN}${2//?/=}${NC}" | tee -a "$LOG_FILE"
}

# Fonction de benchmark personnalisÃ© simplifiÃ©
run_benchmark() {
    local name="$1"
    local script="$2"
    local iterations="${3:-5}"

    print_section "ðŸƒ Benchmark: $name" "="

    local start_time=$(date +%s)
    local results=()

    for i in $(seq 1 $iterations); do
        local iter_start=$(date +%s)
        python "$script" > /dev/null 2>&1
        local iter_end=$(date +%s)
        local duration=$((iter_end - iter_start))
        results+=($duration)
        print_status "ItÃ©ration $i/$iterations: ${duration}s"
    done

    local end_time=$(date +%s)
    local total_duration=$((end_time - start_time))

    # Calcul des statistiques simplifiÃ©
    local sum=0
    local min=${results[0]}
    local max=${results[0]}

    for result in "${results[@]}"; do
        sum=$((sum + result))
        if [ $result -lt $min ]; then
            min=$result
        fi
        if [ $result -gt $max ]; then
            max=$result
        fi
    done

    local avg=$((sum / ${#results[@]}))

    # Sauvegarde des rÃ©sultats
    cat >> "$REPORTS_DIR/benchmark_$TIMESTAMP.json" << EOF
{
    "benchmark": "$name",
    "timestamp": "$(date -Iseconds)",
    "iterations": $iterations,
    "total_duration": $total_duration,
    "statistics": {
        "min": $min,
        "max": $max,
        "average": $avg,
        "total": $sum
    },
    "individual_results": [$(IFS=,; echo "${results[*]}")]
}
EOF

    print_success "$name: avg=${avg}s, min=${min}s, max=${max}s"
}

# Fonction de test de charge simplifiÃ©
run_load_test() {
    local name="$1"
    local endpoint="$2"
    local concurrent_users="${3:-10}"
    local duration="${4:-30}"

    print_section "ðŸ”¥ Test de charge: $name" "="

    # DÃ©marrage de l'API si nÃ©cessaire
    if ! curl -s -o /dev/null -w "%{http_code}" "$endpoint" 2>/dev/null | grep -q "[24][0-9][0-9]"; then
        print_status "DÃ©marrage de l'API pour le test de charge..."
        docker compose up -d arkalia-api 2>/dev/null || {
            print_warning "Impossible de dÃ©marrer l'API - test de charge ignorÃ©"
            return 0
        }

        # Attente de l'API
        for i in {1..10}; do
            if curl -s -o /dev/null -w "%{http_code}" "$endpoint" 2>/dev/null | grep -q "[24][0-9][0-9]"; then
                break
            fi
            sleep 2
        done
    fi

    # Test de charge simplifiÃ© avec curl
    local results_file="$REPORTS_DIR/loadtest_${name}_$TIMESTAMP.txt"
    local start_time=$(date +%s)
    local success_count=0
    local total_requests=50

    print_status "ExÃ©cution de $total_requests requÃªtes..."

    for i in $(seq 1 $total_requests); do
        local req_start=$(date +%s.%N)
        if curl -s -o /dev/null -w "%{http_code}" "$endpoint" 2>/dev/null | grep -q "[24][0-9][0-9]"; then
            ((success_count++))
        fi
        local req_end=$(date +%s.%N)
        echo "$req_start,$req_end" >> "$results_file"

        # Petite pause pour Ã©viter la surcharge
        sleep 0.1
    done

    local end_time=$(date +%s)
    local total_duration=$((end_time - start_time))
    local success_rate=$((success_count * 100 / total_requests))

    print_success "$name: $success_count/$total_requests succÃ¨s (${success_rate}%), durÃ©e ${total_duration}s"

    # ArrÃªt de l'API si on l'a dÃ©marrÃ©e
    docker compose stop arkalia-api 2>/dev/null || true
}

# Fonction de test de mÃ©moire simplifiÃ©
run_memory_test() {
    local name="$1"
    local script="$2"

    print_section "ðŸ§  Test de mÃ©moire: $name" "="

    if ! command -v python &> /dev/null; then
        print_warning "Python non trouvÃ© - test de mÃ©moire ignorÃ©"
        return 0
    fi

    # Test basique avec psutil
    python -c "
import psutil
import os
import time
import subprocess

try:
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024

    # ExÃ©cution du script
    start_time = time.time()
    result = subprocess.run(['python', '$script'], capture_output=True, text=True, timeout=30)
    end_time = time.time()

    final_memory = process.memory_info().rss / 1024 / 1024
    duration = end_time - start_time

    print(f'Initial: {initial_memory:.2f}MB, Final: {final_memory:.2f}MB, Duration: {duration:.2f}s')
except Exception as e:
    print(f'Test de mÃ©moire Ã©chouÃ©: {e}')
" 2>/dev/null || print_warning "Test de mÃ©moire Ã©chouÃ©"

    print_success "$name: Test de mÃ©moire terminÃ©"
}

# Fonction de test de CPU simplifiÃ©
run_cpu_test() {
    local name="$1"
    local script="$2"

    print_section "âš¡ Test de CPU: $name" "="

    if ! command -v python &> /dev/null; then
        print_warning "Python non trouvÃ© - test de CPU ignorÃ©"
        return 0
    fi

    # Test avec cProfile
    local profile_file="$REPORTS_DIR/cpu_${name}_$TIMESTAMP.prof"
    python -m cProfile -o "$profile_file" "$script" > /dev/null 2>&1

    # Analyse basique du profil
    python -c "
import pstats
from pstats import SortKey

try:
    p = pstats.Stats('$profile_file')
    p.sort_stats(SortKey.TIME)
    print('Top 3 functions by time:')
    p.print_stats(3)
except Exception as e:
    print(f'Analyse du profil CPU Ã©chouÃ©e: {e}')
" 2>/dev/null || print_warning "Analyse du profil CPU Ã©chouÃ©e"

    print_success "$name: Profil CPU gÃ©nÃ©rÃ©"
}

# Fonction de test de modules spÃ©cifiques
run_module_tests() {
    print_section "ðŸ§ª Tests de Modules SpÃ©cifiques" "="

    # Test ZeroIA
    if [ -f "modules/zeroia/core.py" ]; then
        print_status "Test du module ZeroIA..."
        python -c "
import sys
sys.path.append('.')
try:
    from modules.zeroia.core import ZeroIACore
core = ZeroIACore()
    print('âœ… ZeroIA Core initialisÃ© avec succÃ¨s')
except Exception as e:
    print(f'âŒ Erreur ZeroIA: {e}')
" 2>/dev/null || print_warning "Test ZeroIA Ã©chouÃ©"
    fi

    # Test Reflexia
    if [ -f "modules/reflexia/core.py" ]; then
        print_status "Test du module Reflexia..."
        python -c "
import sys
sys.path.append('.')
try:
    from modules.reflexia.core import launch_reflexia_check
    result = launch_reflexia_check()
    print('âœ… Reflexia check exÃ©cutÃ© avec succÃ¨s')
except Exception as e:
    print(f'âŒ Erreur Reflexia: {e}')
" 2>/dev/null || print_warning "Test Reflexia Ã©chouÃ©"
    fi

    # Test Sandozia
    if [ -f "modules/sandozia/core.py" ]; then
        print_status "Test du module Sandozia..."
        python -c "
import sys
sys.path.append('.')
try:
    from modules.sandozia.core import UsandoziaCore
core = UsandoziaCore()
    print('âœ… Sandozia Core initialisÃ© avec succÃ¨s')
except Exception as e:
    print(f'âŒ Erreur Sandozia: {e}')
" 2>/dev/null || print_warning "Test Sandozia Ã©chouÃ©"
    fi
}

# Fonction principale
main() {
    START_TIME=$(date +%s)

    print_header

    # CrÃ©ation des dossiers
    mkdir -p "$REPORTS_DIR"

    # Nettoyage des anciens rapports
    rm -f "$REPORTS_DIR"/benchmark_*.json "$REPORTS_DIR"/loadtest_*.txt "$REPORTS_DIR"/memory_*.txt "$REPORTS_DIR"/cpu_*.prof

    # Initialisation du fichier de rÃ©sultats
    echo "[" > "$REPORTS_DIR/benchmark_$TIMESTAMP.json"

    print_section "ðŸš€ DÃ‰MARRAGE DES TESTS DE PERFORMANCE" "="

    # Benchmarks des modules principaux
    if [ -f "demo_global.py" ]; then
        run_benchmark "Demo Global" "demo_global.py" 3
    fi

    if [ -f "arkalia_score.py" ]; then
        run_benchmark "Score Cognitif" "arkalia_score.py" 3
    fi

    # Tests de modules spÃ©cifiques
    run_module_tests

    # Tests de charge des APIs (simplifiÃ©s)
    run_load_test "API Reflexia" "http://localhost:8000/reason" 5 20
    run_load_test "API ZeroIA" "http://localhost:8000/zeroia" 5 20

    # Tests de mÃ©moire
    if [ -f "modules/zeroia/core.py" ]; then
        run_memory_test "ZeroIA Core" "modules/zeroia/core.py"
    fi

    if [ -f "modules/reflexia/core.py" ]; then
        run_memory_test "Reflexia Core" "modules/reflexia/core.py"
    fi

    # Tests de CPU
    if [ -f "modules/zeroia/reason_loop.py" ]; then
        run_cpu_test "ZeroIA Reason Loop" "modules/zeroia/reason_loop.py"
    fi

    # Tests de performance pytest
    print_section "ðŸ§ª Tests de Performance Pytest" "="
    pytest -c pytest-performance.ini tests/performance || print_warning "Tests de performance pytest Ã©chouÃ©s"

    # Fermeture du fichier JSON
    echo "]" >> "$REPORTS_DIR/benchmark_$TIMESTAMP.json"

    # GÃ©nÃ©ration du rapport
    print_section "ðŸ“Š GÃ‰NÃ‰RATION DU RAPPORT" "="

    cat > "$REPORTS_DIR/performance_summary_$TIMESTAMP.md" << EOF
# ðŸŒ• Rapport de Performance Arkalia-LUNA

**Date :** $(date)
**DurÃ©e totale :** $(($(date +%s) - START_TIME))s

## ðŸ“‹ Benchmarks exÃ©cutÃ©s

$(grep "âœ….*Benchmark:" "$LOG_FILE" | tail -10)

## ðŸ”¥ Tests de charge

$(grep "âœ….*Test de charge:" "$LOG_FILE" | tail -5)

## ðŸ§  Tests de mÃ©moire

$(grep "âœ….*Test de mÃ©moire:" "$LOG_FILE" | tail -5)

## âš¡ Tests de CPU

$(grep "âœ….*Test de CPU:" "$LOG_FILE" | tail -5)

## ðŸ“ Fichiers gÃ©nÃ©rÃ©s

- \`$REPORTS_DIR/benchmark_$TIMESTAMP.json\` - RÃ©sultats des benchmarks
- \`$REPORTS_DIR/loadtest_*.txt\` - RÃ©sultats des tests de charge
- \`$REPORTS_DIR/memory_*.txt\` - RÃ©sultats des tests de mÃ©moire
- \`$REPORTS_DIR/cpu_*.prof\` - Profils CPU
- \`$LOG_FILE\` - Log complet
- \`$REPORTS_DIR/performance_summary_$TIMESTAMP.md\` - Ce rapport

EOF

    print_success "Rapport de performance gÃ©nÃ©rÃ©"

    # RÃ©sumÃ© final
    print_section "ðŸŽ¯ RÃ‰SUMÃ‰ FINAL" "="
    print_status "DurÃ©e totale : $(($(date +%s) - START_TIME))s"
    print_success "ðŸŒ• Tests de performance terminÃ©s"
    print_status "ðŸ“Š Rapports dans : $REPORTS_DIR/"
}

# Gestion des signaux
trap 'print_warning "Interruption dÃ©tectÃ©e - ArrÃªt en cours..."; exit 1' INT TERM

# ExÃ©cution
main "$@"
