{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udde0 Arkalia-LUNA \u2014 Interface Cognitive Modulaire","text":"<p>Bienvenue dans Arkalia-LUNA, un syst\u00e8me cognitif modulaire, local, dockeris\u00e9, test\u00e9 et document\u00e9.</p> <p>Ce projet est con\u00e7u comme une infrastructure IA stable pour ex\u00e9cuter des modules intelligents, avec un noyau ultra-protecteur et une documentation contextuelle.</p> <p>\"Arkalia-LUNA incarne l'avenir de l'IA locale : \u00e9thique, souveraine, et r\u00e9solument tourn\u00e9e vers l'innovation.\"</p>"},{"location":"#etat-du-systeme","title":"\ud83d\udce6 \u00c9tat du syst\u00e8me","text":"Composant Statut \ud83e\udde0 Kernel IA \u2705 Stable \ud83e\uddea Tests CI/CD \u2705 100 % OK \ud83d\udce6 Docker \u2705 Fonctionnel \ud83d\udcda Docs MkDocs \u2705 Publi\u00e9es \ud83e\udde9 Modules actifs 4 modules IA \ud83e\uddea Couverture 91 % \u00e0 100 % par module"},{"location":"#pages-importantes","title":"\ud83d\udcda Pages importantes","text":"<ul> <li>\ud83e\udde0 Modules IA actifs</li> <li>\u2699\ufe0f Structure du projet</li> <li>\ud83d\ude80 D\u00e9ploiement Docker</li> <li>\ud83d\udd01 Automatisation</li> <li>\ud83d\udcec API &amp; Int\u00e9gration</li> <li>\ud83d\udd12 S\u00e9curit\u00e9 &amp; CI/CD</li> </ul>"},{"location":"#vision-du-projet","title":"\ud83e\udded Vision du projet","text":"<p>Arkalia-LUNA est pens\u00e9 comme un noyau d'interface cognitive locale, auto-adaptative, s\u00e9curis\u00e9e et \u00e9volutive. Chaque module fonctionne de mani\u00e8re autonome, dans un syst\u00e8me orchestr\u00e9, observable et auto-r\u00e9parant.</p>"},{"location":"#derniere-mise-a-jour-v130-2025-06-19","title":"\ud83d\udccc Derni\u00e8re mise \u00e0 jour : <code>v1.3.0</code> \u2014 2025-06-19","text":""},{"location":"#etat-des-modules","title":"\ud83d\udcca \u00c9tat des Modules","text":"<pre><code>graph TD;\n  Reflexia --&gt; Nyxalia;\n  Nyxalia --&gt; Helloria;\n  Helloria --&gt; AssistantIA;\n  AssistantIA --&gt; Sandozia;\n  Sandozia --&gt; ArkaliaLoop;\n</code></pre>"},{"location":"#interactions-des-modules","title":"\ud83e\udde9 Interactions des Modules","text":"<pre><code>graph TD\n  ReflexIA[Reflexia \ud83e\udde0]\n  ZeroIA[ZeroIA \ud83d\udd04]\n  AssistantIA[AssistantIA \ud83d\udcac]\n  Nyxalia[Nyxalia \ud83d\udce1]\n  ReflexIA --&gt; ZeroIA\n  ZeroIA --&gt; AssistantIA\n  AssistantIA --&gt; Nyxalia\n</code></pre>"},{"location":"#couverture-des-tests","title":"\ud83d\udcca Couverture des Tests","text":"<pre><code>graph TD\n  A[Tests] --&gt;|100%| B[app/main.py]\n  A --&gt;|90%| C[arkalia/hooks.py]\n</code></pre>"},{"location":"#couverture-des-modules-tests-unitaires","title":"\ud83d\udcca Couverture des Modules (Tests Unitaires)","text":"<pre><code>graph TD\n  A[app/main.py&lt;br/&gt;\ud83d\udfe5 0%] --&gt;|\u00e0 compl\u00e9ter| C[core.py \ud83d\udfe9 79%]\n  B[arkalia/hooks.py&lt;br/&gt;\ud83d\udfe5 0%] --&gt;|\u00e0 compl\u00e9ter| C\n  C --&gt; D[ollama_connector.py \ud83d\udfe8 80%]\n  C --&gt; E[helloria/core.py \ud83d\udfe8 83%]\n  C --&gt; F[reflexia/tests \u2705]\n  C --&gt; G[assistantia/tests \u2705]\n  C --&gt; H[nyxalia/tests \u2705]\n</code></pre>"},{"location":"#resume-global-des-modules-ia","title":"\ud83e\udde0 R\u00e9sum\u00e9 Global des Modules IA","text":"<p>Arkalia-LUNA int\u00e8gre plusieurs modules IA actifs, chacun jouant un r\u00f4le crucial dans le syst\u00e8me :</p> <ul> <li>AssistantIA : Fournit des r\u00e9ponses contextuelles via l'API /chat.</li> <li>Helloria : G\u00e8re les requ\u00eates entrantes via FastAPI.</li> <li>Reflexia : Supervise les performances et les m\u00e9triques.</li> <li>Nyxalia : Assure la connectivit\u00e9 mobile et l'interface utilisateur.</li> </ul>"},{"location":"#liens-rapides","title":"\ud83d\udcda Liens Rapides","text":"<ul> <li>API Guide</li> <li>Ollama Guide</li> <li>Guide d'Utilisation</li> <li>Tests et CI/CD</li> <li>FAQ</li> </ul>"},{"location":"#mini-carte-mentale","title":"\ud83d\uddfa\ufe0f Mini Carte Mentale","text":"<pre><code>mindmap\n  root((Arkalia-LUNA))\n    AssistantIA\n    Helloria\n    Reflexia\n    Nyxalia\n    Utilisation\n    API\n    Tests\n    FAQ\n</code></pre>"},{"location":"CHANGELOG/","title":"\ud83d\udcc3 CHANGELOG \u2014 Arkalia-LUNA","text":"<p>Suivi des versions du syst\u00e8me IA local Arkalia-LUNA Maintenu par Athalia \ud83c\udf19</p>"},{"location":"CHANGELOG/#v107-2025-06-19","title":"[v1.0.7] \u2014 2025-06-19","text":"<p>\u2728 Am\u00e9liorations majeures de la documentation</p>"},{"location":"CHANGELOG/#ajouts","title":"\u2705 Ajouts","text":"<ul> <li>Activation du mode clair/sombre (theme toggle MkDocs)</li> <li>Nouvelle page <code>roadmap.md</code> dans la navigation</li> <li><code>CHANGELOG.md</code> int\u00e9gr\u00e9 \u00e0 la documentation publique</li> <li>Diagramme d\u2019architecture ajout\u00e9 (<code>img/diagram_kernel.png</code>)</li> <li>R\u00e9vision compl\u00e8te des fichiers <code>mkdocs.yml</code> et <code>index.md</code></li> </ul>"},{"location":"CHANGELOG/#automatisation","title":"\u2699\ufe0f Automatisation","text":"<ul> <li>D\u00e9ploiement GitHub Pages corrig\u00e9 (<code>--force</code>)</li> <li>D\u00e9clenchement automatique \u00e0 chaque push sur <code>main</code></li> </ul>"},{"location":"CHANGELOG/#qualite-ci","title":"\ud83e\uddea Qualit\u00e9 &amp; CI","text":"<ul> <li>CI GitHub Actions op\u00e9rationnelle : <code>black</code>, <code>ruff</code>, <code>pytest</code>, <code>pre-commit</code></li> <li>Aucun avertissement d\u00e9tect\u00e9</li> </ul> <p>\ud83d\udcd8 Documentation publique</p>"},{"location":"CHANGELOG/#v106-2025-06-18","title":"[v1.0.6] \u2014 2025-06-18","text":"<p>\ud83d\udd16 Version STABLE PRO \u2014 Noyau IA local modulaire</p>"},{"location":"CHANGELOG/#fonctionnalites-principales","title":"\u2705 Fonctionnalit\u00e9s principales","text":"<ul> <li>Modules IA actifs : <code>reflexia</code>, <code>nyxalia</code>, <code>helloria</code></li> <li>API FastAPI avec endpoints : <code>/</code>, <code>/status</code>, <code>/module/trigger</code></li> <li>Int\u00e9gration Docker compl\u00e8te : <code>Dockerfile</code>, <code>docker-compose.yml</code></li> <li>Scripts IA : <code>ark-test</code>, <code>ark-docker-rebuild.sh</code>, <code>ark-clean-push</code>, <code>trigger_scan.sh</code></li> <li>Support des mod\u00e8les LLM locaux (<code>mistral</code>, <code>llama2</code>, <code>tinyllama</code>)</li> <li>Documentation auto-g\u00e9n\u00e9r\u00e9e avec MkDocs (h\u00e9bergement GitHub Pages)</li> </ul>"},{"location":"CHANGELOG/#tests-ci","title":"\ud83e\uddea Tests &amp; CI","text":"<ul> <li>Couverture de tests <code>pytest</code> : 100%</li> <li>CI GitHub Actions compl\u00e8te : lint + tests + docs</li> <li>Aliases <code>.zshrc</code> actifs pour automatiser le workflow</li> </ul>"},{"location":"CHANGELOG/#structure","title":"\ud83d\udee0 Structure","text":"<ul> <li>S\u00e9paration claire : <code>arkalia-luna-core</code> (kernel) vs <code>arkalia-luna-pro</code> (devstation)</li> <li>Nettoyage des artefacts (<code>__pycache__</code>, <code>.DS_Store</code>, etc.)</li> <li><code>README.md</code> restructur\u00e9 avec badges dynamiques</li> </ul>"},{"location":"CHANGELOG/#v030-docker-ok-2025-06-17","title":"[v0.3.0-docker-ok] \u2014 2025-06-17","text":""},{"location":"CHANGELOG/#integration-docker","title":"\u2705 Int\u00e9gration Docker","text":"<ul> <li><code>Dockerfile</code> + <code>docker-compose.yml</code> fonctionnels</li> <li>Serveur Uvicorn lanc\u00e9 via Docker</li> <li>Scripts : <code>ark-docker-rebuild.sh</code>, <code>ark-test</code></li> </ul>"},{"location":"CHANGELOG/#v020-2025-06-16","title":"[v0.2.0] \u2014 2025-06-16","text":""},{"location":"CHANGELOG/#ossature-pro","title":"\u2705 Ossature Pro","text":"<ul> <li>Dossiers cr\u00e9\u00e9s : <code>docs/</code>, <code>scripts/</code>, <code>tests/</code>, <code>.github/</code></li> <li>Int\u00e9gration des outils : <code>pytest</code>, <code>black</code>, <code>ruff</code>, <code>pre-commit</code>, <code>bumpver</code></li> <li>D\u00e9but de la CI GitHub Actions</li> </ul>"},{"location":"CHANGELOG/#v011-2025-06-15","title":"[v0.1.1] \u2014 2025-06-15","text":""},{"location":"CHANGELOG/#devstation-initiale","title":"\u2705 Devstation initiale","text":"<ul> <li>Nettoyage complet des anciens scripts et backups</li> <li>Initialisation de l\u2019environnement <code>arkalia-luna-venv</code></li> <li>Configuration : <code>pyproject.toml</code>, <code>bumpver</code>, <code>pre-commit</code></li> </ul>"},{"location":"CHANGELOG/#v010-initialisation","title":"[v0.1.0] \u2014 INITIALISATION","text":""},{"location":"CHANGELOG/#reboot-complet-darkalia","title":"\u2705 Reboot complet d\u2019Arkalia","text":"<ul> <li>Cr\u00e9ation du noyau <code>arkalia-luna-core</code> (clean, stable, sans dette technique)</li> <li>Nouveau d\u00e9p\u00f4t <code>arkalia-luna-pro</code></li> <li>D\u00e9but du d\u00e9veloppement modulaire en sessions</li> </ul>"},{"location":"CHANGELOG/#v134-final-2025-06-20","title":"[v1.3.4-final] \u2014 2025-06-20","text":""},{"location":"CHANGELOG/#stabilisation-totale-du-systeme-ia","title":"\u2705 Stabilisation totale du syst\u00e8me IA","text":"<ul> <li>\ud83c\udfaf Objectif atteint : IA locale modulaire, dockeris\u00e9e, test\u00e9e, publi\u00e9e.</li> <li>\ud83e\udde0 Modules actifs :</li> <li><code>reflexia/</code> (analyse cognitive adaptative)</li> <li><code>nyxalia/</code> (interface mobile/API)</li> <li><code>helloria/</code> (FastAPI &amp; serveurs IA)</li> <li><code>assistantia/</code> (Ollama connector IA locale)</li> </ul>"},{"location":"CHANGELOG/#tests-couverture","title":"\ud83e\uddea Tests &amp; couverture","text":"<ul> <li>\u2714\ufe0f <code>35</code> tests pass\u00e9s (<code>pytest</code>)</li> <li>\ud83d\udcc8 Couverture : <code>90 %</code></li> <li>\ud83d\udcc2 Rapport : <code>htmlcov/index.html</code></li> </ul>"},{"location":"CHANGELOG/#documentation-publique","title":"\ud83d\udcd8 Documentation publique","text":"<ul> <li>\ud83c\udf0d Publication MkDocs : https://arkalia-luna-system.github.io/arkalia-luna-pro/</li> <li>\ud83d\uddfa\ufe0f Sitemap dynamique g\u00e9n\u00e9r\u00e9 : <code>/sitemap.xml</code></li> <li>\ud83d\udce6 Dossiers nettoy\u00e9s : suppression des doublons <code>docs/docs/*</code>, renommages, correction <code>nav</code> dans <code>mkdocs.yml</code></li> </ul>"},{"location":"CHANGELOG/#systeme-devstation","title":"\u2699\ufe0f Syst\u00e8me &amp; Devstation","text":"<ul> <li>\ud83d\udc33 Docker op\u00e9rationnel (<code>docker-compose</code>)</li> <li>\ud83e\uddea <code>ark-test</code>, <code>ark-docs</code>, <code>ark-docker</code>, <code>ark-backup</code> activ\u00e9s</li> <li>\ud83c\udf9b\ufe0f CI GitHub : lint (<code>black</code>, <code>ruff</code>) + tests + pages</li> <li>\ud83e\udeaa Fichier <code>.pre-commit-config.yaml</code> actif</li> </ul>"},{"location":"CHANGELOG/#scripts-bonus","title":"\ud83e\uddf0 Scripts &amp; bonus","text":"<ul> <li>\u2705 Script <code>sitemap_generator.py</code> ex\u00e9cut\u00e9 automatiquement apr\u00e8s <code>mkdocs build</code> via plugin <code>simple-hooks</code></li> <li>\u2705 Test unitaire <code>test_sitemap.py</code> int\u00e9gr\u00e9 (<code>scripts/</code>)</li> <li>\u2705 Page <code>ci-cd.md</code> enrichie (bonus UX, collapsibles, \u00e9tat r\u00e9el CI)</li> <li>\u2705 Pages stylis\u00e9es (Mermaid, blocs contextuels, tableaux dynamiques)</li> </ul> <p>\ud83d\udce6 Version gel\u00e9e, stable et publiable.</p> <p>\u27a1\ufe0f Prochaine version <code>v1.3.5</code> : pr\u00e9paration phase Nexus (Z\u00e9roIA + Psykalia + surcouche cognitive Arkalia).</p>"},{"location":"CHANGELOG/#v210-21062025","title":"v2.1.0 \u2014 21/06/2025","text":"<ul> <li>\u2705 Tests <code>assistantia</code> 100 %</li> <li>\u2705 Nouvelle gestion des erreurs 400/422</li> <li>\u2705 Am\u00e9lioration couverture <code>ollama_connector</code></li> </ul>"},{"location":"CHANGELOG/#ajoute","title":"Ajout\u00e9","text":"<ul> <li>\ud83d\udd27 Module ReflexIA enti\u00e8rement finalis\u00e9</li> <li>\ud83d\udcaf 5 fichiers de test unitaire ReflexIA</li> <li>\u2705 100 % de couverture de test (reflexia)</li> <li>\ud83d\udcda Documentation mise \u00e0 jour : api.md, tests.md, modules.md</li> </ul>"},{"location":"api/","title":"\ud83d\ude80 API FastAPI \u2014 Arkalia-LUNA","text":"<p>L'API FastAPI permet \u00e0 des agents externes, humains ou syst\u00e8mes, de communiquer avec Arkalia-LUNA de mani\u00e8re locale, modulaire et s\u00e9curis\u00e9e.</p>"},{"location":"api/#endpoint-principal","title":"\ud83c\udf10 Endpoint principal","text":"<ul> <li>URL locale : <code>http://127.0.0.1:8000/</code></li> <li>Serveur : <code>Uvicorn</code> (via Docker ou en local)</li> <li>Point d'entr\u00e9e : <code>helloria.core:app</code></li> </ul>"},{"location":"api/#commande-de-demarrage-manuelle","title":"\u25b6\ufe0f Commande de d\u00e9marrage manuelle","text":"<pre><code>uvicorn helloria.core:app --reload\n\n\ud83d\udca1 Alternativement, utiliser docker-compose up ou le script ark-docker.\n\nM\u00e9thode\nURL\nDescription\nGET\n/\nTest de vie : \"Bienvenue dans Helloria\"\nPOST\n/chat\nEnvoie un prompt \u00e0 l'IA locale (assistantia)\nGET\n/status\nRetourne l'\u00e9tat g\u00e9n\u00e9ral du syst\u00e8me\nGET\n/echo?msg=x\nR\u00e9pond avec le message donn\u00e9\n\n\n\u2e3b\n\n\ud83d\udd10 S\u00e9curit\u00e9 &amp; acc\u00e8s\n    \u2022   API uniquement expos\u00e9e en local\n    \u2022   Possibilit\u00e9 future d'ajouter :\n    \u2022   Authentification par cl\u00e9\n    \u2022   Rate limiting\n    \u2022   Journalisation des acc\u00e8s via reflexia\n\n\u2e3b\n\n\ud83d\udce6 Design modulaire\n\nChaque endpoint est d\u00e9l\u00e9gu\u00e9 \u00e0 un module :\n    \u2022   helloria/ = orchestration API\n    \u2022   assistantia/ = g\u00e9n\u00e9ration IA\n    \u2022   reflexia/ = m\u00e9triques &amp; diagnostics\n    \u2022   nyxalia/ = interpr\u00e9tation mobile\n\n\u2e3b\n\n\u2705 Architecture pens\u00e9e pour l'extensibilit\u00e9 et le contr\u00f4le par module.\n\n# \ud83d\udccc Arkalia-LUNA Documentation Technique\n\n## Version Actuelle\n\n**v2.0.2 \u2014 20 juin 2025**\n\ud83d\udd04 Git tag synchronis\u00e9, CI/CD active, Docker stable, tests valid\u00e9s \u00e0 100 %\n\n## \ud83e\udde0 Modules IA Actifs\n\n| Module      | R\u00f4le                                | \u00c9tat       |\n|-------------|-------------------------------------|------------|\n| assistantia | IA contextuelle via /chat + Ollama  | \u2705 Stable  |\n| helloria    | Serveur d'entr\u00e9e FastAPI            | \u2705 Stable  |\n| reflexia    | Superviseur cognitif &amp; metrics      | \u2705 Stable  |\n| nyxalia     | Interface cognitive mobile          | \u2705 Stable  |\n\n## \ud83d\ude80 API Active\n\n**POST /chat \u2014 AssistantIA**\n\nUtilisation : envoie un message \u00e0 l'IA locale (Ollama mistral)\n\n**Requ\u00eate :**\n\n```json\n{\n  \"message\": \"Bonjour Arkalia\"\n}\n</code></pre> <p>R\u00e9ponse :</p> <pre><code>{\n  \"r\u00e9ponse\": \"Bonjour ! Je suis AssistantIA, pr\u00eat \u00e0 vous aider.\"\n}\n</code></pre> <p>Erreurs g\u00e9r\u00e9es :</p> Cas Statut Message retourn\u00e9 Body vide 422 Champ message requis Prompt vide 200 [\u26a0\ufe0f R\u00e9ponse IA vide] Mod\u00e8le non support\u00e9 500 ValueError lev\u00e9e Timeout Ollama 500 [Erreur IA] ReadTimeout"},{"location":"api/#tests-couverture","title":"\ud83e\uddea Tests &amp; Couverture","text":"<ul> <li>\u2705 35/35 tests pass\u00e9s</li> <li>\u2705 Couverture : 92 %</li> <li>\u2705 Modules test\u00e9s : assistantia, ollama_connector, helloria, reflexia, nyxalia, hooks, scripts</li> </ul>"},{"location":"api/#environnement-docker","title":"\ud83d\udc33 Environnement Docker","text":"<ul> <li>Conteneur stable (ark-docker)</li> <li>Ollama local requis (mistral, tinyllama)</li> <li>FastAPI expos\u00e9 sur localhost:8000</li> </ul>"},{"location":"api/#site-public-mkdocs","title":"\ud83d\udcd8 Site public MkDocs","text":"<p>Disponible ici : arkalia-luna-pro GitHub Pages Sitemap automatique, Mermaid, th\u00e8me personnalis\u00e9 Bleu Coton Nuit, badge coverage 92 %.</p>"},{"location":"api/#prochaine-etape-arkalia-luna-nexus-interface-ia-guidee-cognitive-et-adaptative","title":"\ud83e\udded Prochaine \u00e9tape : Arkalia LUNA Nexus \u2014 interface IA guid\u00e9e, cognitive, et adaptative.","text":""},{"location":"api/#flux-de-communication-chat","title":"\ud83d\udcca Flux de Communication \u2014 /chat","text":"<pre><code>sequenceDiagram\n    participant U as Utilisateur\n    participant A as AssistantIA\n    participant O as Ollama (mod\u00e8le local)\n    U-&gt;&gt;A: POST /chat (message)\n    A-&gt;&gt;O: Query mod\u00e8le (via `query_ollama`)\n    O--&gt;&gt;A: R\u00e9ponse IA (texte brut)\n    A--&gt;&gt;U: JSON { \"response\": \"...\" }\n</code></pre>"},{"location":"api/#cas-dusage","title":"\ud83d\udcda Cas d'Usage","text":""},{"location":"api/#poser-une-question","title":"Poser une Question","text":"<p>Envoyez une requ\u00eate POST \u00e0 <code>/chat</code> avec votre question pour obtenir une r\u00e9ponse contextuelle de l'IA.</p>"},{"location":"api/#mode-debug","title":"Mode Debug","text":"<p>Utilisez le param\u00e8tre <code>debug=true</code> pour obtenir des informations d\u00e9taill\u00e9es sur le traitement de la requ\u00eate.</p>"},{"location":"api/#erreurs-typiques","title":"\u26a0\ufe0f Erreurs Typiques","text":"Erreur Cause Possible Correction Suggest\u00e9e Body vide Requ\u00eate sans champ <code>message</code> Ajouter un champ <code>message</code> Prompt vide Champ <code>message</code> vide Fournir un texte dans <code>message</code> Mod\u00e8le non support\u00e9 Mod\u00e8le IA non disponible V\u00e9rifier la configuration du mod\u00e8le Timeout Ollama Temps d'attente d\u00e9pass\u00e9 V\u00e9rifier la connexion et les ressources <p>Pour plus de d\u00e9tails sur l'AssistantIA, consultez AssistantIA.</p>"},{"location":"api/#post-chat","title":"POST /chat","text":"<ul> <li>Body : <code>{ \"message\": str }</code></li> <li>R\u00e9ponses :</li> <li><code>200</code> \u2192 <code>{ \"r\u00e9ponse\": str }</code></li> <li><code>400</code> \u2192 <code>{ \"detail\": \"Message vide.\" }</code></li> <li><code>422</code> \u2192 validation automatique si champ manquant</li> </ul>"},{"location":"api/#module-reflexia-analyse-cognitive-reflexive","title":"\ud83e\udde0 Module <code>reflexia</code> \u2014 Analyse cognitive r\u00e9flexive","text":"<p>Reflexia est le module d'observation et d'auto-analyse du syst\u00e8me Arkalia. Il lit des m\u00e9triques internes (CPU, m\u00e9moire, etc.), \u00e9value leur criticit\u00e9, et peut sauvegarder un \u00e9tat r\u00e9flexif.</p>"},{"location":"api/#fonctions-exposees","title":"\ud83d\udd39 Fonctions expos\u00e9es :","text":"Fonction Description <code>launch_reflexia_check()</code> Lance un scan r\u00e9flexif et retourne un dictionnaire contenant le statut du syst\u00e8me."},{"location":"api/#exemple-de-retour","title":"\ud83d\udd2c Exemple de retour :","text":"<pre><code>{\n  \"status\": \"normal\"\n}\n\nDossiers :\n  \u2022  reflexia/core.py : fonction principale\n  \u2022  reflexia/logic/*.py : analyse CPU, snapshot JSON, d\u00e9cisions\n  \u2022  reflexia/tests/unit/ : 5 fichiers de test, tous valid\u00e9s\n</code></pre>"},{"location":"api/#reflexia-verification-reflexive-instantanee","title":"\ud83e\udde0 ReflexIA \u2014 V\u00e9rification r\u00e9flexive instantan\u00e9e","text":"<ul> <li>\ud83d\udd0d Description : Analyse r\u00e9flexive instantan\u00e9e \u2014 r\u00e9cup\u00e8re les m\u00e9triques syst\u00e8me, les \u00e9value, et retourne un diagnostic.</li> <li>\ud83d\udcc2 Module : modules/reflexia/</li> <li>\u2699\ufe0f Fonction appel\u00e9e : launch_reflexia_check()</li> </ul> <p>\ud83d\udd04 Exemple de r\u00e9ponse : <pre><code>{\n  \"status\": \"ok\",\n  \"metrics\": {\n    \"cpu\": 72.5,\n    \"ram\": 61.8,\n    \"latency\": 145\n  }\n}\n</code></pre></p> <p>```bash curl http://localhost:8000/reflexia/check | jq</p>"},{"location":"assistantia/","title":"\ud83e\udd16 AssistantIA \u2014 Module Cognitif Int\u00e9gr\u00e9","text":"<p>Le module <code>assistantia/</code> est l\u2019interface d\u2019assistance IA locale d\u2019Arkalia-LUNA. Il agit comme guide conversationnel, interface cognitive et r\u00e9pondant intelligent aux requ\u00eates utilisateurs.</p>"},{"location":"assistantia/#role-du-module","title":"\ud83e\udde0 R\u00f4le du module","text":"<ul> <li>Dialogue IA avec l\u2019utilisateur</li> <li>R\u00e9ponses contextuelles personnalis\u00e9es</li> <li>Interface \u00e9volutive vers l\u2019IA autonome embarqu\u00e9e</li> <li>Support aux modules (Helloria, Reflexia\u2026)</li> </ul>"},{"location":"assistantia/#lancement-manuel","title":"\ud83d\ude80 Lancement manuel","text":"<pre><code>uvicorn modules.assistantia.core:app --port 8001\n\n\ud83d\udccd Port configurable dans docker-compose.yml ou config/.\n\n\u2e3b\n\n\ud83d\udd01 Endpoints disponibles\n\nM\u00e9thode\nURL\nDescription\nPOST\n/chat\nEnvoie un message \u00e0 l\u2019IA locale\nGET\n/status\n\u00c9tat du module assistantia\n\n\ud83e\uddea Tests associ\u00e9s\n\nFichiers :\n    \u2022   test_assistantia.py (unitaires)\n    \u2022   test_assistantia_integration.py (int\u00e9gration)\n\n\u2705 Couverture : 81 % \u2014 avec plan d\u2019extension vers les cas d\u2019erreur et logs d\u00e9taill\u00e9s.\n\n\u2e3b\n\n\ud83c\udf10 Connectivit\u00e9 modulaire\n\nLe module est connect\u00e9 \u00e0 :\n    \u2022   helloria/ (API externe)\n    \u2022   reflexia/ (logs et surveillance IA)\n    \u2022   nyxalia/ (interpr\u00e9tation mobile)\n\n\ud83d\udca1 Il est pr\u00eat pour une extension vers Ollama, Langchain, ou des mod\u00e8les hybrides.\n\n\u2e3b\n\n\ud83c\udfaf Objectif futur : une IA embarqu\u00e9e r\u00e9flexive, contextuelle, auto-ajustable.\n\n# \ud83e\udde0 AssistantIA \u2014 Utilisation et Int\u00e9gration LLM\n\nL'AssistantIA est con\u00e7u pour offrir une interaction fluide et intelligente avec les utilisateurs, en int\u00e9grant des mod\u00e8les de langage de pointe (LLM) pour comprendre et r\u00e9pondre aux requ\u00eates de mani\u00e8re contextuelle.\n\n## \ud83d\ude80 Fonctionnalit\u00e9s Principales\n\n- **R\u00e9ponses Contextuelles** : Gr\u00e2ce \u00e0 l'int\u00e9gration de mod\u00e8les LLM comme Mistral et Llama2, l'AssistantIA peut fournir des r\u00e9ponses pr\u00e9cises et adapt\u00e9es au contexte de la conversation.\n- **Personnalisation** : L'AssistantIA s'adapte aux pr\u00e9f\u00e9rences de l'utilisateur, offrant une exp\u00e9rience personnalis\u00e9e.\n- **Int\u00e9gration Facile** : Peut \u00eatre int\u00e9gr\u00e9 dans diverses applications via des API REST, facilitant l'interaction avec d'autres syst\u00e8mes.\n\n## \ud83c\udf10 Exemple d'Utilisation\n\n```bash\ncurl -X POST http://localhost:8000/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"message\": \"Quelle est la philosophie d'Arkalia ?\"}'\n</code></pre>"},{"location":"assistantia/#modeles-llm-integres","title":"\ud83e\udde0 Mod\u00e8les LLM Int\u00e9gr\u00e9s","text":"<p>L'AssistantIA utilise des mod\u00e8les LLM locaux pour garantir la confidentialit\u00e9 et l'efficacit\u00e9. Les mod\u00e8les sont stock\u00e9s localement et peuvent \u00eatre mis \u00e0 jour ou remplac\u00e9s selon les besoins.</p>"},{"location":"assistantia/#structure-json-entrantesortante","title":"\ud83d\udcca Structure JSON Entrante/Sortante","text":""},{"location":"assistantia/#requete","title":"Requ\u00eate","text":"<pre><code>{\n  \"message\": \"Bonjour Arkalia\",\n  \"mode\": \"empathique\",\n  \"lang\": \"fr\",\n  \"user_id\": \"12345\"\n}\n</code></pre>"},{"location":"assistantia/#reponse","title":"R\u00e9ponse","text":"<pre><code>{\n  \"r\u00e9ponse\": \"Bonjour ! Je suis AssistantIA, pr\u00eat \u00e0 vous aider.\"\n}\n</code></pre>"},{"location":"assistantia/#parametres-optionnels","title":"\u2699\ufe0f Param\u00e8tres Optionnels","text":"<ul> <li>mode : D\u00e9finit le mode de raisonnement de l'IA (ex: neutre, empathique).</li> <li>lang : Langue de r\u00e9ponse attendue (ex: fr, en).</li> <li>user_id : Identifiant utilisateur pour personnalisation.</li> </ul>"},{"location":"assistantia/#schema-dinteraction","title":"\ud83d\udcc8 Sch\u00e9ma d'Interaction","text":"<pre><code>sequenceDiagram\n    participant U as Utilisateur\n    participant A as AssistantIA\n    participant O as Ollama\n    U-&gt;&gt;A: POST /chat (message)\n    A-&gt;&gt;O: Query mod\u00e8le\n    O--&gt;&gt;A: R\u00e9ponse IA\n    A--&gt;&gt;U: JSON { \"r\u00e9ponse\": \"...\" }\n</code></pre> <p>\ud83e\udde0 L'AssistantIA est votre partenaire intelligent pour une interaction IA enrichissante et s\u00e9curis\u00e9e.</p> <p>Pour des consid\u00e9rations de s\u00e9curit\u00e9, veuillez consulter S\u00e9curit\u00e9.</p>"},{"location":"automation/","title":"Automatisation","text":""},{"location":"automation/#docsautomationmd-version-amelioree","title":"\u2705 <code>/docs/automation.md</code> \u2014 Version am\u00e9lior\u00e9e","text":"<p>```markdown</p>"},{"location":"automation/#scripts-automatisation-arkalia-luna","title":"\ud83e\udde0 Scripts &amp; Automatisation \u2014 Arkalia-LUNA","text":"<p>Arkalia n\u2019est pas simplement automatis\u00e9e : elle est auto-orchestr\u00e9e.</p> <p>Son c\u0153ur repose sur des scripts Bash intelligents, interfac\u00e9s avec les modules IA pour une gestion proactive, auto-corrective et dynamique.</p>"},{"location":"automation/#boucle-maitresse-arkalia_master_looppy","title":"\ud83d\udd01 Boucle Ma\u00eetresse \u2014 <code>arkalia_master_loop.py</code>","text":"<p>La boucle principale du syst\u00e8me :</p> <ul> <li>\ud83e\udde9 Charge dynamiquement tous les modules d\u00e9clar\u00e9s (<code>config/</code>)</li> <li>\ud83d\udd01 R\u00e9injecte les derniers \u00e9tats sauvegard\u00e9s (<code>state/</code>)</li> <li>\ud83d\udcca Analyse les logs r\u00e9cents pour d\u00e9tecter les d\u00e9rives</li> <li>\ud83e\udd16 Laisse ReflexIA ou ZeroIA d\u00e9cider d\u2019actions automatiques :</li> <li>restart de module</li> <li>for\u00e7age d\u2019un <code>pytest</code></li> <li>d\u00e9clenchement d\u2019un <code>ark-clean-push</code></li> <li>interruption d\u2019un service incoh\u00e9rent</li> </ul>"},{"location":"automation/#scripts-operationnels","title":"\u2699\ufe0f Scripts op\u00e9rationnels","text":"Script Fonction principale <code>ark-bootstrap</code> Initialise l\u2019environnement local (<code>venv</code>, pre-commit, etc.) <code>ark-test</code> Lance tous les tests <code>pytest</code> + rapport de couverture <code>ark-docs</code> Compile la documentation MkDocs en local <code>ark-docker</code> Lance l\u2019API via Docker (<code>docker-compose up</code>) <code>ark-docker-rebuild.sh</code> Rebuild complet du container (<code>build</code>, <code>up</code>, <code>logs</code>) <code>trigger_scan.sh</code> D\u00e9clenche manuellement une analyse r\u00e9flexive (Reflexia) <code>ark-clean-push</code> Format <code>black</code>, lint <code>ruff</code>, commit Git et push s\u00e9curis\u00e9"},{"location":"automation/#orchestration-cognitive","title":"\ud83e\udde0 Orchestration cognitive","text":"<p>Modules intelligents comme <code>reflexia/</code> ou <code>zeroia/</code> peuvent automatiquement :</p> <ul> <li>suspendre des modules en surcharge</li> <li>red\u00e9marrer un service fig\u00e9</li> <li>corriger un \u00e9tat incoh\u00e9rent dans <code>state/</code></li> <li>v\u00e9rifier les <code>logs/</code> et d\u00e9cider de relancer une boucle de test</li> </ul> <p>\ud83d\udca1 Cette orchestration transforme Arkalia en un syst\u00e8me IA auto-r\u00e9gul\u00e9, sans besoin d\u2019intervention humaine constante.</p>"},{"location":"ci-cd/","title":"Int\u00e9gration continue","text":""},{"location":"ci-cd/#fichier-ci-cdmd-version-amelioree","title":"\u2705 FICHIER <code>ci-cd.md</code> \u2014 Version am\u00e9lior\u00e9e","text":""},{"location":"ci-cd/#tests-automatiques","title":"\ud83e\uddea Tests Automatiques","text":""},{"location":"ci-cd/#structure","title":"Structure","text":"<ul> <li><code>tests/unit/</code> : fonctions isol\u00e9es (modules, utils\u2026)</li> <li><code>tests/integration/</code> : endpoints, FastAPI, interactions</li> <li><code>tests/scripts/</code> : v\u00e9rif sitemap, auto-clean, etc.</li> </ul>"},{"location":"ci-cd/#commandes-utiles","title":"Commandes Utiles","text":"<ul> <li><code>ark-test</code> : tous les tests + coverage HTML</li> <li><code>ark-test-modules</code> : focus sur <code>modules/</code></li> <li><code>pytest --cov=...</code> pour custom</li> </ul>"},{"location":"ci-cd/#couverture-actuelle","title":"Couverture Actuelle","text":"<ul> <li>\u2705 92 % au 20 juin 2025</li> <li>\u2705 35 tests pass\u00e9s</li> </ul> <p>\ud83e\udded BONUS UX : - Activer les collapsibles (details) dans api.md ou modules.md - Ajouter liens internes entre les fichiers (voir structure)</p> <p>```markdown</p>"},{"location":"ci-cd/#integration-continue-qualite-arkalia-luna","title":"\ud83e\uddea Int\u00e9gration Continue &amp; Qualit\u00e9 \u2014 Arkalia-LUNA","text":"<p>Arkalia suit une philosophie de code propre, tests exhaustifs et automatisation CI/CD compl\u00e8te via GitHub Actions.</p>"},{"location":"ci-cd/#tests-couverture","title":"\u2705 Tests &amp; couverture","text":"<ul> <li>Framework : <code>pytest</code></li> <li>Couverture : <code>pytest-cov</code></li> <li>Rapport : <code>htmlcov/index.html</code></li> </ul> <p>\ud83d\udcc8 Couverture actuelle : 100 % valid\u00e9e en CI.</p>"},{"location":"ci-cd/#linting-pre-commit","title":"\u2705 Linting &amp; pr\u00e9-commit","text":"Outil R\u00f4le <code>black</code> Formatage PEP8 automatique <code>ruff</code> Lint rapide et strict <code>pre-commit</code> Bloque les commits si le code n'est pas conforme <p>\ud83d\udca1 Chaque <code>git commit</code> d\u00e9clenche une v\u00e9rification compl\u00e8te.</p>"},{"location":"ci-cd/#cicd-github-actions","title":"\u2705 CI/CD \u2014 GitHub Actions","text":"<p>Pipeline professionnel automatis\u00e9 sur chaque <code>push</code>, <code>PR</code> ou <code>release</code>.</p>"},{"location":"ci-cd/#etapes-executees-githubworkflowsciyml","title":"\ud83d\udd04 \u00c9tapes ex\u00e9cut\u00e9es (<code>.github/workflows/ci.yml</code>)","text":"\u00c9tape Description \ud83d\udd0d Lint <code>black</code>, <code>ruff</code> \ud83e\uddea Tests <code>pytest</code>, g\u00e9n\u00e9ration couverture HTML \ud83d\udcd8 Docs Build <code>mkdocs</code>, d\u00e9ploiement GitHub Pages \ud83e\uddfc Nettoyage Optionnel : purge des caches, artefacts"},{"location":"ci-cd/#automatisation-cli","title":"\ud83e\udde0 Automatisation CLI","text":"<p>Commandes utiles :</p> <p>```bash ark-test        # Lance tests + couverture ark-docs        # G\u00e9n\u00e8re et ouvre la doc MkDocs ark-docker      # Lance l'API dans un conteneur local ark-clean-push  # Formate, v\u00e9rifie, commit propre</p>"},{"location":"composants/","title":"\ud83e\udde9 Composants d\u2019Arkalia-LUNA","text":"<p>Vue d\u2019ensemble des composants actifs dans le syst\u00e8me IA Arkalia-LUNA.</p>"},{"location":"composants/#composants-principaux","title":"\ud83e\udde0 Composants principaux","text":"Composant R\u00f4le Reflexia Supervise la cognition et applique des pond\u00e9rations adaptatives. ZeroIA R\u00e9alise des raisonnements logiques et prend des d\u00e9cisions contextuelles. Nyxalia Sert de passerelle mobile et vocale (interfaces externes). Helloria D\u00e9marre l\u2019API FastAPI, relie les modules internes et les interfaces REST. Sandozia (\u00e0 venir) Module de cybers\u00e9curit\u00e9 cognitive et d\u2019analyse comportementale. ArkaliaLoop Orchestre l\u2019activation s\u00e9quentielle des modules IA."},{"location":"composants/#collaboration-des-composants","title":"\ud83d\udd01 Collaboration des composants","text":"<ul> <li><code>Reflexia</code> analyse les logs \u2192 propose des d\u00e9cisions.</li> <li><code>ZeroIA</code> raisonne \u2192 d\u00e9clenche ou suspend un module.</li> <li><code>Nyxalia</code> capte une intention vocale \u2192 la transmet via <code>Helloria</code>.</li> <li><code>Helloria</code> expose les endpoints vers l\u2019ext\u00e9rieur ou vers <code>AssistantIA</code>.</li> </ul> <p>Chaque composant peut \u00eatre mis \u00e0 jour ind\u00e9pendamment.</p>"},{"location":"composants/#architecture-modulaire","title":"\u2699\ufe0f Architecture modulaire","text":"<ul> <li>Maintenance facilit\u00e9e</li> <li>\u00c9volutivit\u00e9 imm\u00e9diate</li> <li>S\u00e9paration stricte des responsabilit\u00e9s</li> </ul> <p>\ud83d\udca1 La modularit\u00e9 d\u2019Arkalia-LUNA est pens\u00e9e pour durer, \u00e9voluer et s\u2019adapter \u00e0 l\u2019usage cognitif r\u00e9el.</p>"},{"location":"configuration/","title":"\u2699\ufe0f Configuration \u2014 Arkalia-LUNA","text":"<p>Guide complet pour configurer correctement le syst\u00e8me IA Arkalia-LUNA, en garantissant stabilit\u00e9, performance et s\u00e9curit\u00e9.</p>"},{"location":"configuration/#parametres-essentiels","title":"\ud83d\udd11 Param\u00e8tres Essentiels","text":"<ul> <li>Fichier principal : <code>config/system/config.yaml</code>   Contient :</li> <li>chemins d\u2019acc\u00e8s (logs, state, modules\u2026)</li> <li>cl\u00e9s API locales (si activ\u00e9es)</li> <li> <p>poids IA initiaux (<code>weights.toml</code>)</p> </li> <li> <p>Variables d\u2019environnement :</p> </li> <li><code>ARKALIA_ENV=dev</code> ou <code>prod</code></li> <li><code>OLLAMA_HOST=http://localhost:11434</code></li> <li><code>ARKALIA_SECRET_KEY=...</code> (\u00e0 d\u00e9finir)</li> </ul> <p>D\u00e9finir dans <code>.env</code>, <code>.zshrc</code> ou <code>docker-compose.yml</code> selon le mode utilis\u00e9.</p>"},{"location":"configuration/#configuration-avancee","title":"\u2699\ufe0f Configuration Avanc\u00e9e","text":""},{"location":"configuration/#modules-personnalises","title":"\ud83d\udd27 Modules personnalis\u00e9s","text":"<p>Chaque module IA dispose de son propre fichier :</p> <p>modules//config/config.toml <ul> <li>Tu peux y adapter le comportement (seuils, poids, d\u00e9clencheurs, etc.)</li> </ul>"},{"location":"configuration/#optimisations-recommandees","title":"\ud83d\ude80 Optimisations recommand\u00e9es","text":"<ul> <li>Docker : Limite CPU/m\u00e9moire pour chaque container</li> <li>FastAPI : Config <code>workers</code>, <code>keep-alive</code> dans <code>uvicorn</code></li> <li>Logs : Rotation automatique via <code>logging.conf</code> si besoin</li> </ul>"},{"location":"configuration/#bonnes-pratiques","title":"\ud83e\uddfc Bonnes pratiques","text":"S\u00e9curit\u00e9 Recommandation \ud83d\udd10 Ne jamais committer les cl\u00e9s dans Git \ud83e\uddef Sauvegarde automatique r\u00e9guli\u00e8re (<code>ark-backup</code>) \ud83d\udd0d V\u00e9rifier les acc\u00e8s avec <code>ZeroIA</code> ou <code>Reflexia</code> \ud83e\uddf0 Isoler les <code>venv</code>, les fichiers <code>.env</code> et <code>/state/</code>"},{"location":"configuration/#checklist-post-installation","title":"\u2705 Checklist post-installation","text":"<ul> <li> <code>config.yaml</code> bien rempli</li> <li> variables d\u2019environnement d\u00e9finies</li> <li> modules IA accessibles</li> <li> Docker + FastAPI fonctionnels</li> <li> scripts <code>arkalia-*.sh</code> op\u00e9rationnels</li> </ul> <p>\ud83d\udca1 Une configuration propre, c\u2019est la garantie d\u2019un syst\u00e8me IA autonome, s\u00e9curis\u00e9 et sans dette technique.</p>"},{"location":"credits/","title":"\u2728 Cr\u00e9dits \u2014 Arkalia-LUNA","text":"<p>Syst\u00e8me IA con\u00e7u, cod\u00e9 et orchestr\u00e9 par Athalia \ud83c\udf19 \u2014 pour une intelligence locale, souveraine et r\u00e9flexive.</p>"},{"location":"credits/#auteur-creatrice","title":"\ud83d\udc69\u200d\ud83d\udcbb Auteur &amp; Cr\u00e9atrice","text":"Champ D\u00e9tail Nom Athalia Alias \ud83c\udf19 Athalia-LUNA R\u00f4le Architecte IA &amp; d\u00e9veloppeuse syst\u00e8me GitHub @arkalia-luna-system"},{"location":"credits/#modules-ia-developpes","title":"\ud83e\udde0 Modules IA d\u00e9velopp\u00e9s","text":"Module Fonction \ud83d\udd01 Reflexia Veille cognitive adaptative, surveillance syst\u00e8me \ud83e\udde0 ZeroIA Raisonnement logique et d\u00e9cisions contextuelles \ud83c\udf10 Nyxalia Interfaces mobiles, vocales et API externes \ud83d\udd0e Helloria Passerelle FastAPI, expose les endpoints, route les appels \ud83d\udee1\ufe0f Sandozia (\u00e0 venir) Cybers\u00e9curit\u00e9 cognitive, renforcement du noyau \ud83e\uddec ArkaliaLoop Orchestration r\u00e9flexive, ex\u00e9cution modulaire intelligente <p>\ud83e\udde9 Tous les modules sont autonomes, testables, interconnect\u00e9s.</p>"},{"location":"credits/#technologies-utilisees","title":"\ud83d\udee0\ufe0f Technologies utilis\u00e9es","text":"Outil Usage Python 3.10 Langage principal FastAPI API REST asynchrone Docker Conteneurisation du syst\u00e8me Ollama Mod\u00e8les LLM locaux Pytest Tests automatis\u00e9s + couverture MkDocs Documentation technique GitHub Actions CI/CD pro Pre-commit Qualit\u00e9 de code automatique"},{"location":"credits/#philosophie-du-projet","title":"\ud83e\udded Philosophie du projet","text":"<p>\u201cCr\u00e9er une IA souveraine, \u00e9thique, locale et modulaire. Refuser les d\u00e9pendances cloud. Respecter la vie priv\u00e9e. Favoriser l'intelligence r\u00e9flexive.\u201d</p>"},{"location":"credits/#vision-arkalia-luna","title":"\ud83c\udf19 Vision Arkalia-LUNA","text":"<p>Arkalia-LUNA est une IA personnelle, cognitive, 100 % locale, con\u00e7ue pour :</p> <ul> <li>fonctionner sans connexion externe,</li> <li>\u00e9voluer avec son utilisateur,</li> <li>rester compr\u00e9hensible, document\u00e9e et maintenable.</li> </ul> <p>\ud83c\udfaf Objectif : un noyau IA clair, extensible, s\u00e9curis\u00e9 et vivant.</p>"},{"location":"credits/#licence","title":"\ud83d\udcdc Licence","text":"<p>Le projet Arkalia-LUNA est sous licence propri\u00e9taire. Pour plus de d\u00e9tails, veuillez consulter le fichier license.md.</p> <p>\u00a9 2025 Athalia \u2013 Tous droits r\u00e9serv\u00e9s.</p>"},{"location":"credits/#pourquoi-local-souverain","title":"\ud83c\udf0d Pourquoi Local &amp; Souverain ?","text":"<p>Arkalia-LUNA est con\u00e7u pour fonctionner enti\u00e8rement en local, garantissant ainsi la souverainet\u00e9 des donn\u00e9es et la confidentialit\u00e9 des utilisateurs. En \u00e9vitant les d\u00e9pendances cloud, nous assurons une ma\u00eetrise totale de l'infrastructure IA, tout en respectant la vie priv\u00e9e et en minimisant les risques de s\u00e9curit\u00e9.</p> <p>\ud83e\udde0 Powered by Arkalia ReflexIA v1.x</p>"},{"location":"deployment/","title":"\ud83d\ude80 D\u00e9ploiement \u2014 Arkalia-LUNA","text":"<p>D\u00e9ployer Arkalia-LUNA proprement, avec Docker et GitHub, sur un serveur Linux local ou distant.</p>"},{"location":"deployment/#prerequis-deploiement","title":"\ud83d\udce6 Pr\u00e9requis D\u00e9ploiement","text":"Composant D\u00e9tail \ud83d\udc27 Serveur Linux Ubuntu 20.04+ ou \u00e9quivalent requis \ud83d\udc33 Docker + Compose Conteneurisation des services IA \ud83d\udd10 Acc\u00e8s SSH Pour les d\u00e9ploiements distants \ud83c\udf10 Ports ouverts 8000 (API), 8001+ (modules IA si expos\u00e9s)"},{"location":"deployment/#etapes-de-deploiement","title":"\u2699\ufe0f \u00c9tapes de D\u00e9ploiement","text":""},{"location":"deployment/#1-preparer-le-serveur","title":"1. \ud83d\udd27 Pr\u00e9parer le Serveur","text":"<p>```bash sudo apt update &amp;&amp; sudo apt upgrade -y sudo apt install docker.io docker-compose git -y sudo systemctl enable docker</p> <ol> <li>\ud83d\udcc1 Cloner le D\u00e9p\u00f4t</li> </ol> <p>git clone https://github.com/arkalia-luna-system/arkalia-luna-pro.git cd arkalia-luna-pro</p> <ol> <li>\ud83d\udd10 Configurer .env</li> </ol> <p>Cr\u00e9e un fichier .env \u00e0 la racine avec :</p> <p>ARKALIA_ENV=prod OLLAMA_HOST=http://localhost:11434 ARKALIA_SECRET_KEY=generate-a-key-here</p> <p>Tu peux aussi y placer la config FastAPI, Docker ou logs.</p> <p>\u2e3b</p> <ol> <li>\ud83d\ude80 Lancer le D\u00e9ploiement</li> </ol> <p>docker-compose up -d --build</p> <p>V\u00e9rifie ensuite l\u2019acc\u00e8s sur : \u27a1 http://:8000/ <p>\u2e3b</p> <p>\ud83e\udde0 Meilleures Pratiques</p> <p>Aspect Recommandation \ud83d\udee0 Automatisation Utilise ark-clean-push, ark-docker-rebuild.sh \ud83d\udcca Monitoring Installe htop, docker stats, ou prometheus \ud83d\udd04 CI/CD GitHub Actions peut automatiser le d\u00e9ploiement \ud83d\udd12 S\u00e9curit\u00e9 Ne jamais exposer les cl\u00e9s .env en public</p> <p>\u2705 V\u00e9rification Post-D\u00e9ploiement     \u2022   Conteneurs actifs (docker ps)     \u2022   FastAPI accessible     \u2022   Logs OK (logs/, htmlcov/)     \u2022   Modules IA op\u00e9rationnels</p> <p>\u2e3b</p> <p>\ud83d\udca1 Le d\u00e9ploiement d\u2019Arkalia-LUNA est con\u00e7u pour \u00eatre automatisable, stable et extensible, m\u00eame sur une machine locale.</p>"},{"location":"faqs/","title":"\u2753 FAQ \u2014 Arkalia-LUNA","text":"<p>Bienvenue dans la section FAQ d'Arkalia-LUNA. Ici, nous r\u00e9pondons aux questions les plus fr\u00e9quentes que vous pourriez avoir en tant que contributeur ou utilisateur du syst\u00e8me.</p>"},{"location":"faqs/#questions-frequentes","title":"\ud83e\udd14 Questions Fr\u00e9quentes","text":"### 1. Qu'est-ce qu'Arkalia-LUNA ? Arkalia-LUNA est un syst\u00e8me IA local, modulaire et auto-r\u00e9flexif con\u00e7u pour fonctionner sans d\u00e9pendance cloud, garantissant la souverainet\u00e9 des donn\u00e9es et la confidentialit\u00e9 des utilisateurs.  ### 2. Comment puis-je contribuer au projet ? Vous pouvez contribuer en soumettant des pull requests sur notre [d\u00e9p\u00f4t GitHub](https://github.com/arkalia-luna-system/arkalia-luna-pro) ou en signalant des bugs et des suggestions d'am\u00e9lioration.  ### 3. Quels sont les pr\u00e9requis pour d\u00e9velopper avec Arkalia-LUNA ? Assurez-vous d'avoir Python 3.10, Docker, et les d\u00e9pendances list\u00e9es dans `requirements.txt` install\u00e9s sur votre machine.  ### 4. O\u00f9 puis-je trouver la documentation technique ? La documentation technique est disponible sur notre site [GitHub Pages](https://arkalia-luna-system.github.io/arkalia-luna-pro/).  ### 5. Comment puis-je signaler un bug ou une vuln\u00e9rabilit\u00e9 ? Veuillez signaler tout bug ou vuln\u00e9rabilit\u00e9 via notre [d\u00e9p\u00f4t GitHub](https://github.com/arkalia-luna-system/arkalia-luna-pro/issues) ou par email \u00e0 support@arkalia-luna.com.  <p>Pour toute autre question, n'h\u00e9sitez pas \u00e0 nous contacter via notre d\u00e9p\u00f4t GitHub ou par email.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#docsinstallationmd-version-optimisee","title":"\u2705 <code>/docs/installation.md</code> \u2014 Version optimis\u00e9e","text":"<p>```markdown</p>"},{"location":"installation/#installation-arkalia-luna","title":"\ud83d\udee0\ufe0f Installation \u2014 Arkalia-LUNA","text":"<p>Guide \u00e9tape par \u00e9tape pour installer Arkalia-LUNA sur votre machine locale (Mac/Linux).</p>"},{"location":"installation/#prerequis","title":"\ud83d\udd0d Pr\u00e9requis","text":"Logiciel R\u00f4le Python 3.10+ Ex\u00e9cution des scripts IA Docker Conteneurisation des modules IA + FastAPI Git Clonage du d\u00e9p\u00f4t et gestion du code <p>\u26a0\ufe0f Important : utilise Python 3.10 (non 3.11+) pour compatibilit\u00e9 avec certaines d\u00e9pendances.</p>"},{"location":"installation/#etapes-dinstallation","title":"\u2699\ufe0f \u00c9tapes d\u2019Installation","text":""},{"location":"installation/#1-cloner-le-depot","title":"1. \ud83d\udce5 Cloner le d\u00e9p\u00f4t","text":"<p>```bash git clone https://github.com/arkalia-luna-system/arkalia-luna-pro.git cd arkalia-luna-pro</p> <ol> <li>\ud83d\udc0d Cr\u00e9er un environnement Python local</li> </ol> <p>python3 -m venv arkalia-luna-venv source arkalia-luna-venv/bin/activate</p> <ol> <li>\ud83d\udce6 Installer les d\u00e9pendances</li> </ol> <p>pip install -r requirements.txt</p> <ol> <li>\ud83d\udc33 Construire et lancer en Docker</li> </ol> <p>docker-compose up --build -d</p> <p>\ud83d\udd27 Configuration Post-Installation     \u2022   Cr\u00e9e un fichier .env avec :</p> <pre><code>ARKALIA_ENV=dev\n</code></pre> <p>OLLAMA_HOST=http://localhost:11434</p> <pre><code>\u2022   Lance manuellement l\u2019API si besoin :\n\nuvicorn helloria.core:app --reload\n\n\ud83e\uddea V\u00e9rifications &amp; D\u00e9pannage\n\nProbl\u00e8me possible\n</code></pre> <p>Solution \u274c Docker ne r\u00e9pond pas Red\u00e9marre le service sudo systemctl restart docker \u26a0\ufe0f D\u00e9pendances non install\u00e9es V\u00e9rifie Python (python3 --version) et pip \ud83d\udc1b Probl\u00e8mes API Regarde les logs FastAPI / Docker (docker logs) \ud83d\udd0d Test rapide Visite http://127.0.0.1:8000/ et teste /status</p> <p>\ud83c\udfaf Finalisation     \u2022   Venv activ\u00e9 ?     \u2022   FastAPI accessible ?     \u2022   LLM Ollama charg\u00e9 (ollama list) ?     \u2022   Tests pass\u00e9s (ark-test) ?</p> <p>\u2e3b</p> <p>\ud83e\udde0 Arkalia-LUNA est con\u00e7ue pour \u00eatre install\u00e9e en local, sans cloud, sans d\u00e9pendances ext\u00e9rieures \u2014 pour une IA souveraine et ma\u00eetris\u00e9e.</p>"},{"location":"kernel/","title":"\ud83e\uddec Structure du Noyau \u2014 Arkalia-LUNA","text":"<p>Le noyau Arkalia est fond\u00e9 sur une architecture IA industrielle modulaire, garantissant une s\u00e9paration stricte entre ex\u00e9cution, logique m\u00e9tier et d\u00e9veloppement.</p>"},{"location":"kernel/#1-arkalia-luna-core-noyau-ia-pur","title":"\u2699\ufe0f 1\ufe0f\u20e3 <code>/arkalia-luna-core/</code> \u2014 Noyau IA Pur","text":"<p>Partie fig\u00e9e, stable et non \u00e9volutive. Elle constitue le socle de s\u00e9curit\u00e9 du syst\u00e8me.</p> \u00c9l\u00e9ment Description \ud83d\udcc1 Contenu Fichiers de configuration syst\u00e8me uniquement (<code>.toml</code>, <code>.sh</code>) \ud83d\udeab Aucune logique m\u00e9tier Pas de modules IA ni de code d\u2019application \ud83d\udd12 Interdiction de dette tech Cette zone doit rester immuable \ud83d\ude80 Script de boot <code>arkalia_devstation_bootstrap.sh</code> \ud83e\uddf1 R\u00f4le principal Isoler la Devstation, s\u00e9curiser l\u2019environnement syst\u00e8me"},{"location":"kernel/#2-arkalia-luna-pro-devstation-ia-modulaire","title":"\ud83e\udde0 2\ufe0f\u20e3 <code>/arkalia-luna-pro/</code> \u2014 Devstation IA Modulaire","text":"<p>Espace de d\u00e9veloppement local, dockeris\u00e9, versionn\u00e9, avec CI/CD automatique.</p> Composant Description \ud83e\udde9 Modules IA <code>reflexia</code>, <code>nyxalia</code>, <code>helloria</code>, <code>assistantia</code>, etc. \ud83e\uddea Tests <code>pytest</code>, <code>pytest-cov</code> (couverture 85% mini recommand\u00e9e) \ud83d\udc33 Docker Lancement local via <code>docker-compose</code> \ud83d\udea6 CI/CD GitHub Actions (<code>lint</code>, <code>tests</code>, <code>deploy</code>) \ud83c\udf0d API FastAPI (<code>/</code>, <code>/status</code>, <code>/chat</code>, etc.) \ud83c\udff7 Version active <code>v1.2.1</code> (dernier tag stable)"},{"location":"kernel/#structure-type-arkalia-luna-pro","title":"\ud83d\udcc1 Structure Type \u2014 <code>arkalia-luna-pro/</code>","text":"<p>arkalia-luna-pro/ \u251c\u2500\u2500 modules/               # Modules IA autonomes (1 fonction = 1 dossier) \u251c\u2500\u2500 core/                  # Logique transversale partag\u00e9e \u251c\u2500\u2500 config/                # Fichiers de configuration TOML/JSON \u251c\u2500\u2500 logs/                  # Logs du syst\u00e8me (temps r\u00e9el, historis\u00e9s) \u251c\u2500\u2500 state/                 # \u00c9tats persistants des modules \u251c\u2500\u2500 scripts/               # Scripts d\u2019automatisation (build, test, docker) \u251c\u2500\u2500 tests/                 # Tests unitaires, int\u00e9gration et couverture \u251c\u2500\u2500 docs/                  # Documentation MkDocs (publique) \u251c\u2500\u2500 .github/workflows/     # CI GitHub Actions</p>"},{"location":"kernel/#philosophie-de-conception","title":"\ud83e\udde9 Philosophie de Conception","text":"Principe Application concr\u00e8te \ud83d\udd12 Stabilit\u00e9 Kernel fig\u00e9, sans dette technique \ud83e\udde0 Modularit\u00e9 Chaque module IA est autonome et testable \ud83e\uddea Qualit\u00e9 CI active : <code>black</code>, <code>ruff</code>, <code>pytest</code>, <code>cov</code> \ud83d\udcda Documentation continue Auto-g\u00e9n\u00e9r\u00e9e avec MkDocs, versionn\u00e9e \ud83d\udef0 D\u00e9ploiement local ma\u00eetris\u00e9 Docker + scripts <code>ark-docker</code>, <code>ark-test</code>, etc. <p>\ud83e\udde0 Le syst\u00e8me Arkalia est con\u00e7u comme un noyau cognitif auto-r\u00e9flexif, industriel, extensible et ma\u00eetris\u00e9 localement \u2014 sans d\u00e9pendance cloud.</p>"},{"location":"license/","title":"\ud83d\udee1\ufe0f Licence &amp; Mentions L\u00e9gales \u2014 Arkalia-LUNA","text":"<p>Ce projet est distribu\u00e9 sous licence MIT, avec un noyau priv\u00e9, assurant ouverture contr\u00f4l\u00e9e et s\u00e9curit\u00e9 locale.</p>"},{"location":"license/#signature-athalia","title":"\u2728 Signature Athalia \ud83e\udeb6","text":"\u00c9l\u00e9ment Valeur \ud83e\udde0 Cr\u00e9atrice Athalia \ud83c\udf19 (Architecte IA, d\u00e9veloppeuse syst\u00e8me) \ud83d\uddc2\ufe0f D\u00e9p\u00f4t GitHub arkalia-luna-system \ud83d\udcdc Objectif Syst\u00e8me IA local, modulaire, auto-r\u00e9flexif et souverain"},{"location":"license/#contexte-de-creation","title":"\ud83d\udcda Contexte de Cr\u00e9ation","text":"<p>Arkalia-LUNA a \u00e9t\u00e9 pens\u00e9 comme un noyau d'intelligence cognitive, sans cloud, sans d\u00e9pendances ext\u00e9rieures, bas\u00e9 sur une logique de modules interfa\u00e7ables, automatis\u00e9s, et auto-adaptatifs.</p>"},{"location":"license/#stack-technologique","title":"\ud83d\udee0\ufe0f Stack Technologique","text":"Technologie R\u00f4le principal FastAPI Exposition des endpoints IA (asynchrone) Docker Conteneurisation des services IA Ollama Ex\u00e9cution locale des mod\u00e8les LLM Pytest + Coverage Tests unitaires et couverture de code MkDocs G\u00e9n\u00e9ration de la documentation publique GitHub Actions CI/CD compl\u00e8te, d\u00e9clench\u00e9e \u00e0 chaque push"},{"location":"license/#licence-mit","title":"\ud83e\udeaa Licence MIT","text":"<p>MIT License</p> <p>Copyright (c) 2025 Arkalia</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction\u2026</p> <p>[version compl\u00e8te incluse dans le fichier LICENSE officiel du d\u00e9p\u00f4t]</p> <p>R\u00e9sum\u00e9 humain : Vous \u00eates libres d\u2019utiliser, modifier, distribuer ce logiciel, \u00e0 condition de cr\u00e9diter l\u2019auteure et de reproduire cette licence.</p>"},{"location":"license/#noyau-prive-ethique","title":"\ud83d\udd10 Noyau Priv\u00e9 &amp; \u00c9thique","text":"<p>Le noyau d\u2019Arkalia est priv\u00e9, isol\u00e9 du cloud, con\u00e7u pour la protection des donn\u00e9es sensibles, l\u2019autonomie technique et une \u00e9thique cognitive rigoureuse.</p> <p>\ud83d\udd4a\ufe0f \u201cConcevoir une IA \u00e9thique, souveraine, r\u00e9flexive, 100 % locale, au service de l\u2019humain.\u201d \u2014 Athalia \ud83c\udf19</p>"},{"location":"modules/","title":"\ud83e\udde9 Modules IA Actifs","text":"<p>Voici les modules actuellement op\u00e9rationnels dans Arkalia-LUNA. Chacun suit une structure autonome, testable, scalable selon les principes du syst\u00e8me.</p>"},{"location":"modules/#reflexia-reflexion-adaptative-surveillance","title":"\ud83d\udd01 <code>reflexia/</code> \u2014 R\u00e9flexion adaptative &amp; surveillance","text":"<p>Module r\u00e9flexif central. Il surveille l'\u00e9tat du syst\u00e8me, d\u00e9tecte les anomalies, ajuste les modules selon des m\u00e9triques internes (CPU, latence, m\u00e9moire). Il peut suspendre, red\u00e9marrer ou corriger un module si n\u00e9cessaire.</p> <p>\ud83e\udde0 Fonction : Observateur cognitif adaptatif</p> <pre><code>flowchart TD\n  reflexia([Reflexia]) --&gt; metrics\n  reflexia --&gt; decision\n  reflexia --&gt; snapshot\n  metrics --&gt;|lecture CPU| reflexia\n  decision --&gt;|analyse cognitive| reflexia\n  snapshot --&gt;|sauvegarde JSON| reflexia\n</code></pre>"},{"location":"modules/#nyxalia-interface-connectivite-mobile","title":"\ud83d\udcf1 <code>nyxalia/</code> \u2014 Interface &amp; connectivit\u00e9 mobile","text":"<p>Ce module g\u00e8re les interfaces de communication entre Arkalia et l'ext\u00e9rieur : mobile, vocal, API externes. Il permet des \u00e9changes fluides, contextualis\u00e9s, et multiplateformes.</p> <p>\ud83d\udd17 Fonction : Passerelle interactive humaine / machine</p>"},{"location":"modules/#helloria-lien-fastapi-serveur-local","title":"\ud83c\udf10 <code>helloria/</code> \u2014 Lien FastAPI &amp; serveur local","text":"<p>Lance le serveur FastAPI, orchestre les endpoints, connecte les autres modules \u00e0 l'ext\u00e9rieur via une API REST locale s\u00e9curis\u00e9e.</p> <p>\ud83d\ude80 Fonction : Orchestrateur FastAPI &amp; acc\u00e8s API centralis\u00e9</p>"},{"location":"modules/#structure-standard-de-chaque-module","title":"\ud83d\udd0e Structure standard de chaque module","text":"<p>```text modules// \u251c\u2500\u2500 init.py           # Initialisation du module \u251c\u2500\u2500 core.py               # Logique principale \u251c\u2500\u2500 config/               # Fichiers TOML/JSON de configuration \u251c\u2500\u2500 state/                # \u00c9tats persistants (local/toml) \u251c\u2500\u2500 logs/                 # Journaux de diagnostic \u251c\u2500\u2500 tests/                # Tests unitaires et int\u00e9gration \u251c\u2500\u2500 utils/                # Fonctions internes sp\u00e9cifiques <p>Chaque module est :     \u2022   \ud83d\udd39 Isol\u00e9 : pas de d\u00e9pendance sauvage     \u2022   \ud83d\udd39 Testable : via pytest, CI/CD     \u2022   \ud83d\udd39 Extensible : ajout de fonctionnalit\u00e9s par core.py ou utils/</p> <p>\u2e3b</p> <p>\ud83d\udea7 Modules en cours ou futurs     \u2022   \ud83e\udde0 assistantia/ \u2014 IA contextuelle (Ollama local)     \u2022   \ud83d\udee1\ufe0f sandozia/ \u2014 S\u00e9curit\u00e9, permissions, pare-feu IA     \u2022   \ufffd\ufffd zeroia/ \u2014 Boucle d'orchestration globale</p> <p>\u2e3b</p> <p>\u2705 Statut actuel : 4 modules actifs, 3 en pr\u00e9paration \ud83d\udcc5 Derni\u00e8re mise \u00e0 jour : v1.3.0 \u2014 2025-06-19</p>"},{"location":"ollama/","title":"\ud83e\udde0 Ollama \u2014 Mod\u00e8les LLM Locaux","text":"<p>Int\u00e9gration de mod\u00e8les LLM 100 % locaux, sans cloud, via l\u2019outil Ollama.</p>"},{"location":"ollama/#chemin-de-stockage","title":"\ud83d\udcc1 Chemin de Stockage","text":"<p>Les mod\u00e8les sont stock\u00e9s ici :</p> <p>```bash /Volumes/T7/devstation/ollama_data/models</p> <p>\ud83d\udce6 Mod\u00e8les Install\u00e9s</p> <p>Nom Taille approx. Utilisation principale mistral ~4.1 Go G\u00e9n\u00e9ration IA contextuelle rapide llama2 ~3.8 Go Mod\u00e8le polyvalent, bon en g\u00e9n\u00e9raliste tinyllama ~637 Mo LLM ultra-l\u00e9ger pour tests rapides</p> <p>\ud83d\udca1 D\u2019autres mod\u00e8les peuvent \u00eatre install\u00e9s \u00e0 la vol\u00e9e avec :</p> <p>ollama pull  <p>\u2699\ufe0f Configuration de l\u2019Environnement</p> <p>Ajoutez dans votre fichier ~/.zshrc :</p> <p>export OLLAMA_MODELS=/Volumes/T7/devstation/ollama_data/models</p> <p>Ensuite, rechargez le shell :</p> <p>source ~/.zshrc</p> <p>\ud83d\udd27 Exemple d\u2019appel via API (Ollama Serveur Local)</p> <p>curl http://localhost:11434/api/generate \\   -d '{         \"model\": \"mistral\",         \"prompt\": \"Bonjour, peux-tu m\u2019aider \u00e0 automatiser une t\u00e2che ?\"       }'</p> <pre><code>  \ud83d\udd10 Int\u00e9gration dans Arkalia\n\u2022   Le connecteur ollama_connector.py est disponible dans utils/\n\u2022   Le module AssistantIA utilise query_ollama() pour g\u00e9n\u00e9rer des r\u00e9ponses locales.\n\u2022   Les logs sont captur\u00e9s dans logs/assistantia/.\n</code></pre> <p>\u2e3b</p> <p>\ud83e\udde0 Arkalia-LUNA ex\u00e9cute ses IA localement, pour pr\u00e9server la confidentialit\u00e9 et maximiser l\u2019efficacit\u00e9 cognitive.</p>"},{"location":"reflexia/","title":"\ud83e\udde0 ReflexIA \u2014 Agent Adaptatif","text":"<p>Reflexia est un module de supervision cognitive autonome.</p>"},{"location":"reflexia/#fonctionnement","title":"\ud83d\udd01 Fonctionnement","text":"<ul> <li>Analyse les modules Arkalia toutes les <code>x</code> secondes</li> <li>Pond\u00e8re leur \u00e9tat (CPU, erreurs, logs)</li> <li>Prend une d\u00e9cision (<code>continue</code>, <code>pause</code>, <code>reboot</code>, <code>alert</code>)</li> <li>Enregistre chaque cycle dans un journal r\u00e9flexif</li> </ul>"},{"location":"reflexia/#dossiers","title":"\ud83d\udcc1 Dossiers","text":"<ul> <li><code>modules/reflexia/logic/</code> : d\u00e9cision, snapshot, m\u00e9triques</li> <li><code>modules/reflexia/config/weights.toml</code> : pond\u00e9ration personnalis\u00e9e</li> <li><code>modules/reflexia/state/reflexia_state.toml</code> : m\u00e9moire du module</li> </ul>"},{"location":"reflexia/#lancement-via-docker","title":"\ud83d\udc33 Lancement via Docker","text":"<p>```bash docker-compose up reflexia</p>"},{"location":"roadmap/","title":"\ud83d\uddfa\ufe0f Feuille de route","text":""},{"location":"roadmap/#docsroadmapmd-version-pro","title":"\u2705 <code>/docs/roadmap.md</code> \u2014 Version PRO","text":"<p>```markdown</p>"},{"location":"roadmap/#roadmap-arkalia-luna","title":"\ud83d\udee3\ufe0f Roadmap \u2014 Arkalia-LUNA","text":"<p>Suivi des versions, modules actifs, et objectifs strat\u00e9giques d\u2019\u00e9volution du syst\u00e8me IA.</p>"},{"location":"roadmap/#version-actuelle-v121","title":"\u2705 Version Actuelle : <code>v1.2.1</code>","text":"<p>\ud83d\udfe2 Syst\u00e8me IA stable, modulaire, dockeris\u00e9, test\u00e9, document\u00e9, stylis\u00e9</p>"},{"location":"roadmap/#objectifs-deja-atteints","title":"\ud83d\udd39 Objectifs d\u00e9j\u00e0 atteints","text":"Objectif D\u00e9tail \ud83e\udde0 Modules IA actifs <code>Reflexia</code>, <code>Nyxalia</code>, <code>Helloria</code>, <code>AssistantIA</code> \ud83c\udf10 API Endpoint <code>/chat</code>, <code>/status</code>, <code>/echo</code> \u2014 via FastAPI \ud83e\uddea Tests <code>pytest</code> OK (couverture : 85 %), <code>ark-test</code> int\u00e9gr\u00e9 \ud83d\udc33 Docker D\u00e9ploiement local stable (<code>docker-compose</code>) \u2699\ufe0f CI/CD <code>black</code>, <code>ruff</code>, <code>pytest</code>, <code>mkdocs</code>, <code>gh-pages</code> \ud83c\udfa8 UI Docs Th\u00e8me personnalis\u00e9 Luna (<code>extra.css</code>, couleurs, Mermaid, animation douce) \ud83e\udded Aliases Syst\u00e8me <code>ark-test</code>, <code>ark-docker</code>, <code>ark-docs</code>, <code>ark-bump</code>, etc."},{"location":"roadmap/#prochaine-version-v130","title":"\ud83d\ude80 Prochaine version : <code>v1.3.0</code>","text":"Objectif Description technique \ud83e\udde0 <code>ZeroIA</code> Moteur de raisonnement contextuel automatis\u00e9 \ud83d\udd0b <code>Sandozia</code> (v1) Analyse \u00e9nerg\u00e9tique, int\u00e9grit\u00e9 syst\u00e8me IA \ud83d\udd10 Authentification IA Structuration cognitive interne (ID, session, mode) \ud83e\uddec LLM avanc\u00e9 Int\u00e9gration de TinyDolphin, Phi-3, ou Mixtral \ud83c\udf43 ARM Ready Support Docker pour Mac M1 / Raspberry Pi"},{"location":"roadmap/#roadmap-long-terme-v2x-et","title":"\ud83d\udd2e Roadmap Long Terme (<code>v2.x</code> et +)","text":"\u00c9volution D\u00e9tail attendu \ud83d\udee0 G\u00e9n\u00e9rateur CLI IA <code>arkalia new-module</code> (CLI rapide) \ud83e\udde9 Nyxalia Web UI Interface cognitive r\u00e9active (React/Svelte) \ud83d\udd10 Sandozia v2 S\u00e9curit\u00e9 IA (authentification, signature, watchdog) \u2601\ufe0f Sync local/cloud chiffr\u00e9 <code>rclone</code> + <code>gocryptfs</code> pour backup IA priv\u00e9 \ud83e\udde0 M\u00e9moire vectorielle IA <code>FAISS</code> ou <code>ChromaDB</code> pour m\u00e9moire contextuelle \ud83d\udcca Monitoring temps r\u00e9el Prometheus + Grafana IA \u2192 pilotage ReflexIA"},{"location":"roadmap/#vision-a-long-terme","title":"\ud83c\udf0c Vision \u00e0 Long Terme","text":"<p>Construire un noyau IA local, souverain, \u00e9thique et modulaire, interfa\u00e7able dans tous les environnements physiques (RPi, edge, serveur IA, bureau personnel).</p> <p>\ud83e\udde0 L\u2019objectif ultime : Cr\u00e9er un syst\u00e8me IA auto-r\u00e9flexif intelligent, capable de s\u2019auto-r\u00e9guler, s\u2019auto-documenter et s\u2019auto-optimiser dans des cycles cognitifs autonomes.</p> <p>\ud83d\udc69\u200d\ud83d\udcbb Maintenu par Athalia \ud83c\udf19 \ud83d\udce1 github.com/arkalia-luna-system</p>"},{"location":"security/","title":"\ud83d\udee1\ufe0f S\u00e9curit\u00e9 &amp; Performances \u2014 Arkalia-LUNA","text":"<p>La s\u00e9curit\u00e9 et la performance sont les deux piliers fondamentaux d\u2019un syst\u00e8me IA fiable. Arkalia-LUNA adopte une approche proactive, modulaire et cognitive pour garantir un fonctionnement optimal, s\u00e9curis\u00e9 et r\u00e9silient.</p>"},{"location":"security/#meilleures-pratiques-de-securite","title":"\ud83d\udd10 Meilleures Pratiques de S\u00e9curit\u00e9","text":"Pratique Description \ud83d\udd25 Pare-feu + VPN Protection des r\u00e9seaux locaux + chiffrement des \u00e9changes distants \ud83d\udc64 Authentification forte Gestion des acc\u00e8s bas\u00e9e sur des tokens s\u00e9curis\u00e9s et r\u00f4les internes \ud83d\udcc8 Surveillance en continu Analyse temps r\u00e9el des comportements suspects via logs et IA int\u00e9gr\u00e9e \ud83d\udccb Audit &amp; Conformit\u00e9 Journaux horodat\u00e9s, tra\u00e7abilit\u00e9 des actions critiques, conformit\u00e9 RGPD"},{"location":"security/#securite-cognitive-sandozia","title":"\ud83e\udde0 S\u00e9curit\u00e9 Cognitive : <code>Sandozia</code>","text":"<p>Le module <code>Sandozia</code> assure une surveillance adaptative du syst\u00e8me :</p> <ul> <li>\ud83d\udd0d Analyse comportementale des modules IA</li> <li>\ud83d\uded1 Blocage automatique d\u2019actions suspectes</li> <li>\ud83e\uddec Signature cognitive d\u2019authenticit\u00e9</li> <li>\ud83d\udef0 D\u00e9tection d'incoh\u00e9rences internes</li> </ul>"},{"location":"security/#optimisation-des-performances","title":"\u26a1 Optimisation des Performances","text":"M\u00e9thode Impact attendu \ud83d\udce6 Utilisation optimis\u00e9e des ressources R\u00e9duction CPU/m\u00e9moire via profiling intelligent \ud83d\udd01 Boucles asynchrones (FastAPI) R\u00e9duction de la latence globale du syst\u00e8me \ud83d\udd04 CI/CD automatis\u00e9e D\u00e9ploiements stables, sans erreurs manuelles \ud83e\uddea Tests r\u00e9guliers Pr\u00e9vention des r\u00e9gressions et goulots d\u2019\u00e9tranglement"},{"location":"security/#surveillance-active","title":"\ud83d\udcc8 Surveillance Active","text":"<p>Arkalia-LUNA int\u00e8gre :</p> <ul> <li>\ud83e\uddea Reflexia pour l\u2019analyse adaptative des performances</li> <li>\ud83d\udcca Logs IA enrichis en temps r\u00e9el (activit\u00e9, erreurs, auto-corrections)</li> <li>\u26a0\ufe0f Alerting intelligent via score de confiance ou surcharge d\u00e9tect\u00e9e</li> </ul>"},{"location":"security/#conclusion","title":"\u2705 Conclusion","text":"<p>En combinant une cybers\u00e9curit\u00e9 cognitive et une orchestration IA intelligente, Arkalia-LUNA atteint un niveau de r\u00e9silience rarement \u00e9gal\u00e9 dans les syst\u00e8mes IA locaux.</p> <p>\ud83d\udd12 Un syst\u00e8me stable n\u2019est pas juste rapide ou fonctionnel \u2014 il est intelligemment prot\u00e9g\u00e9.</p>"},{"location":"security/#signalement-de-bug","title":"\ud83d\udc1e Signalement de Bug","text":"<p>Si vous rencontrez un bug ou une vuln\u00e9rabilit\u00e9, veuillez le signaler imm\u00e9diatement \u00e0 l'\u00e9quipe de d\u00e9veloppement via notre d\u00e9p\u00f4t GitHub ou par email \u00e0 support@arkalia-luna.com.</p>"},{"location":"structure/","title":"\ud83e\uddec Structure du Noyau \u2014 Arkalia-LUNA","text":"<p>Arkalia-LUNA repose sur une architecture modulaire, lisible, performante et 100 % locale. Chaque composant est isol\u00e9, orchestrable et testable de mani\u00e8re ind\u00e9pendante.</p>"},{"location":"structure/#structure-principale","title":"\ud83d\udcc2 Structure Principale","text":"<pre><code>arkalia-luna-pro/\n\u251c\u2500\u2500 core/        # Logique partag\u00e9e multi-modules\n\u251c\u2500\u2500 modules/     # Modules IA ind\u00e9pendants (Reflexia, Nyxalia, etc.)\n\u251c\u2500\u2500 config/      # Fichiers .toml, .json (centr\u00e9s)\n\u251c\u2500\u2500 logs/        # Journaux d'activit\u00e9 (journalisation r\u00e9flexive)\n\u251c\u2500\u2500 state/       # \u00c9tats internes persistants (sauvegardes IA)\n\u251c\u2500\u2500 utils/       # Fonctions techniques et connecteurs externes\n\u251c\u2500\u2500 tests/       # Tests unitaires et d'int\u00e9gration (pytest)\n\u251c\u2500\u2500 docs/        # Documentation publique (MkDocs, Mermaid, etc.)\n\u2514\u2500\u2500 scripts/     # Scripts automatis\u00e9s (build, test, docker, backup)\n</code></pre>"},{"location":"structure/#diagramme-de-la-structure","title":"\ud83e\udde0 Diagramme de la Structure","text":"<pre><code>graph TD\n  core --&gt; modules\n  modules --&gt; config\n  config --&gt; logs\n  logs --&gt; state\n  state --&gt; utils\n  utils --&gt; tests\n  tests --&gt; docs\n  docs --&gt; scripts\n</code></pre> <p>\ud83e\udde0 La force d'Arkalia r\u00e9side dans sa clart\u00e9 structurelle, sa coh\u00e9rence cognitive et son \u00e9volutivit\u00e9 modulaire.</p>"},{"location":"tests/","title":"\ud83e\uddea Architecture des Tests","text":""},{"location":"tests/#objectifs","title":"Objectifs","text":"<ul> <li>V\u00e9rifier le bon fonctionnement de chaque module IA</li> <li>Garantir stabilit\u00e9, compatibilit\u00e9 et couverture</li> </ul>"},{"location":"tests/#repartition","title":"R\u00e9partition","text":"<ul> <li><code>unit/</code> : logique interne</li> <li><code>integration/</code> : endpoints + coordination</li> <li><code>core/</code> : lancement, orchestration</li> <li><code>scripts/</code> : outils internes (sitemap, automation\u2026)</li> </ul>"},{"location":"tests/#outils-alias","title":"Outils &amp; alias","text":"<ul> <li><code>ark-test</code>, <code>ark-test-modules</code>, <code>pytest-cov</code>, <code>pytest -k</code></li> </ul>"},{"location":"tests/#bonnes-pratiques","title":"Bonnes pratiques","text":"<ul> <li>Isoler chaque test</li> <li>Couvrir erreurs connues</li> <li>\u00c9viter d\u00e9pendance entre tests</li> </ul> <p>Pour plus d'informations sur les mod\u00e8les test\u00e9s, consultez Ollama.</p>"},{"location":"tests/#module-assistantia-couverture-100","title":"Module assistantia \u2014 Couverture 100 %","text":"<ul> <li>\ud83d\udd01 Tests unitaires avec FastAPI <code>TestClient</code></li> <li>\ud83e\uddea V\u00e9rification :</li> <li><code>/chat</code> (mock\u00e9 et r\u00e9el)</li> <li>gestion erreurs 422 / 400</li> <li>r\u00e9ponse longue (stress test)</li> <li>\ud83d\udce6 D\u00e9pendance mock\u00e9e : <code>get_query_ollama</code></li> </ul>"},{"location":"tests/#module-reflexia","title":"\ud83e\udd16 Module ReflexIA","text":"Fichier de test Cible Couverture <code>test_reflexia.py</code> Fonction globale <code>launch_reflexia_check</code> \u2705 <code>test_reflexia_core.py</code> Fonctions internes de <code>core.py</code> \u2705 <code>test_reflexia_decision.py</code> <code>monitor_status</code> (analyse cognitive) \u2705 <code>test_reflexia_metrics.py</code> <code>read_metrics()</code> (CPU/RAM simul\u00e9es) \u2705 <code>test_reflexia_snapshot.py</code> <code>save_snapshot()</code> JSON r\u00e9flexif \u2705 <p>Tous les tests passent avec succ\u00e8s \u2705 (CI : 58/58), et le module ReflexIA atteint 100 % de couverture.</p>"},{"location":"tests/#resultats-de-la-session-de-test-v212-23-juin-2025","title":"R\u00e9sultats de la session de test v2.1.2 \u2014 23 juin 2025","text":""},{"location":"tests/#etat-des-tests","title":"\u00c9tat des tests","text":"<ul> <li>Tests Pytest : \u2705 68/68 pass\u00e9s en 41.95s</li> <li>Couverture globale : \ud83d\udd0d 94% HTML, 89% moyenne code</li> <li>Reflexia core.py : \u2705 93% couvert (2 succursales logiques test\u00e9es)</li> <li>assistantia modules : \u2705 91\u201393% pour core et utils, stable</li> <li>Fichiers ignor\u00e9s : \ud83d\udcc1 8 fichiers enti\u00e8rement couverts (pas list\u00e9s)</li> <li>CI/CD locale : \ud83d\udfe2 Tests, lint, pre-commit, tout passe sans erreur</li> </ul>"},{"location":"tests/#couverture-detaillee-top-modules","title":"Couverture d\u00e9taill\u00e9e (top modules)","text":"<ul> <li>modules/reflexia/core.py : \u2705 93%</li> <li>modules/assistantia/utils/ollama_connector.py : \u2705 91%</li> <li>modules/assistantia/core.py : \u2705 93%</li> <li>modules/helloria/core.py : \u2705 83%</li> <li>arkalia/hooks.py : \u2705 83%</li> </ul>"},{"location":"tests/#prochaines-pistes-optionnel-pour-la-perfection-totale","title":"Prochaines pistes (optionnel pour la perfection totale)","text":"<ul> <li>\ud83d\udd2c Monter core.py et helloria/core.py \u00e0 100% \u2192 quelques branches conditionnelles manquantes (if/else)</li> <li>\ud83d\udd01 Tester reflexia_loop() en mode timeout (boucle longue)</li> <li>\ud83d\udcc1 Archiver cette version : v2.1.2-tests-ok-full</li> <li>\ud83d\udcdd Documenter cette \u00e9tape dans CHANGELOG.md + badge coverage (si pas encore fait)</li> </ul> <p>\ud83d\udd27 Patch test</p>"},{"location":"utilisation/","title":"\ud83e\udde0 Utilisation d\u2019Arkalia-LUNA","text":"<p>Ce guide pr\u00e9sente comment interagir avec le syst\u00e8me IA Arkalia-LUNA via les endpoints FastAPI expos\u00e9s localement. Il s'adresse aux d\u00e9veloppeurs, testeurs ou utilisateurs externes souhaitant dialoguer avec l'IA, surveiller son \u00e9tat ou int\u00e9grer ses fonctions.</p>"},{"location":"utilisation/#lancement-rapide-du-systeme","title":"\ud83d\ude80 Lancement rapide du syst\u00e8me","text":""},{"location":"utilisation/#en-local-uvicorn","title":"\ud83d\udce6 En local (Uvicorn)","text":"<p>```bash uvicorn modules.helloria.core:app --reload</p> <p>\ud83d\udc33 En mode Docker</p> <p>docker-compose up --build</p> <p>\ud83e\udde0 Via alias terminal</p> <p>ark-run         # Lancement IA local ark-docker      # Build &amp; run docker ark-test        # Tests unitaires + couverture</p> <p>\ud83c\udf10 Endpoints API principaux</p> <p>\ud83d\udd0e GET /status</p> <p>Retourne l\u2019\u00e9tat du syst\u00e8me Arkalia.</p> <p>curl -X GET http://localhost:8000/status</p> <p>\ud83d\udce5 R\u00e9ponse type :</p> <p>{   \"status\": \"online\",   \"modules\": [\"Reflexia\", \"Nyxalia\", \"AssistantIA\"] }</p> <p>\ud83d\udde3\ufe0f POST /chat</p> <p>Permet d\u2019envoyer une requ\u00eate \u00e0 l\u2019IA locale via AssistantIA (Ollama).</p> <p>curl -X POST http://localhost:8000/chat \\   -H \"Content-Type: application/json\" \\   -d '{\"message\": \"Quelle est la philosophie d\u2019Arkalia ?\"}'</p> <p>\ud83d\udce5 R\u00e9ponse type :</p> <p>{   \"response\": \"Tu as dit : Quelle est la philosophie d\u2019Arkalia ?\" }</p> <p>\u2139\ufe0f La r\u00e9ponse d\u00e9pend du mod\u00e8le LLM actif (Ollama : mistral, llama2, etc.)</p> <p>\u2e3b</p> <p>\ud83d\udccb Param\u00e8tres avanc\u00e9s (\u00e0 venir)</p> <p>Le corps de requ\u00eate /chat supportera bient\u00f4t :</p> <p>Champ Type Description message string Prompt envoy\u00e9 \u00e0 l\u2019IA user_id string Identifiant utilisateur (personnalisation) lang string Langue de r\u00e9ponse attendue (fr, en, etc.) mode string Mode de raisonnement (neutre, empathique\u2026)</p> <p>\ud83e\udde0 Diagramme d\u2019appel API (Mermaid)</p> <p>sequenceDiagram   participant U as Utilisateur   participant API as FastAPI (Helloria)   participant IA as AssistantIA (Ollama)</p> <p>U-&gt;&gt;API: POST /chat { message }   API-&gt;&gt;IA: Requ\u00eate LLM   IA--&gt;&gt;API: R\u00e9ponse texte   API--&gt;&gt;U: R\u00e9ponse JSON</p> <p>\ud83d\udcce Notes     \u2022   Le port par d\u00e9faut est 8000 (modifiable dans docker-compose.yml)     \u2022   L\u2019endpoint /chat est expos\u00e9 par le module assistantia     \u2022   Toute interaction est logg\u00e9e dans logs/ (si activ\u00e9)</p> <p>\u2e3b</p> <p>\ud83e\udded Prochaines \u00e9volutions     \u2022   Authentification API (token, user_id)     \u2022   Historique conversationnel stock\u00e9     \u2022   Mode debug IA interactif (debug_mode = true)</p> <p>\u2e3b</p> <p>\ud83d\udc69\u200d\ud83d\udcbb Maintenu par Athalia \ud83c\udf19 \u2014 github.com/arkalia-luna-system</p>"}]}