---
name: 🚀 Tests de Performance

on:
  push:
    branches: [ main, develop, dev-migration, refonte-stable ]
  pull_request:
    branches: [ main, develop, dev-migration, refonte-stable ]
  schedule:
    # Tests de performance quotidiens à 2h du matin
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: "3.10"
  PERFORMANCE_TIMEOUT: 1800  # 30 minutes

# Permissions pour éviter les erreurs de sécurité
permissions:
  contents: read
  actions: read

jobs:
  # 🚀 Tests de performance complets
  performance-tests:
    name: 📊 Tests de Performance Complets
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.10"]
        test-suite: ["zeroia", "api", "integration"]
      fail-fast: false
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-benchmark pytest-timeout
          pip install aiohttp requests locust

      - name: 🧹 Clean performance artifacts
        run: |
          find . -name "._*" -delete
          find . -name ".DS_Store" -delete
          find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
          rm -rf .benchmarks/ performance-results/

      - name: 🐳 Setup Docker for performance tests
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            image=moby/buildkit:v0.12.0

      - name: 🐳 Start services for performance testing
        run: |
          echo "🚀 Démarrage des services pour tests de performance..."
          docker compose up -d --remove-orphans
          sleep 60

      - name: 🏥 Health check before performance tests
        run: |
          echo "🏥 Vérification santé avant tests de performance..."

          # Vérification des services critiques
          for service in "8000/health" "8000/zeroia/health" "8000/reflexia/health"; do
            echo "🔍 Vérification $service..."
            for i in {1..10}; do
              if curl -f -s "http://localhost:$service" > /dev/null; then
                echo "✅ $service disponible"
                break
              else
                echo "⏳ Tentative $i/10 - $service non disponible"
                sleep 10
              fi
            done
          done

      - name: 🧪 Run Performance Tests - ${{ matrix.test-suite }}
        run: |
          echo "🧪 Exécution des tests de performance - ${{ matrix.test-suite }}..."

          case "${{ matrix.test-suite }}" in
            "zeroia")
              echo "📊 Tests de performance ZeroIA..."
              pytest tests/performance/zeroia/test_zeroia_performance.py::test_zeroia_decision_time_under_2s -v --benchmark-only
              pytest tests/performance/zeroia/test_zeroia_performance.py::test_circuit_breaker_latency_under_10ms -v --benchmark-only
              pytest tests/performance/zeroia/test_zeroia_performance.py::test_performance_regression_detection -v --benchmark-only
              ;;
            "api")
              echo "📊 Tests de performance API..."
              pytest tests/performance/api/ -v --benchmark-only --timeout=300 || echo "⚠️ Tests API terminés avec avertissements"
              ;;
            "integration")
              echo "📊 Tests de performance intégration..."
              pytest tests/performance/integration/ -v --benchmark-only --timeout=600 || echo "⚠️ Tests intégration terminés avec avertissements"
              ;;
          esac

      - name: 📊 Generate performance metrics
        run: |
          echo "📊 Génération des métriques de performance..."

          # Collecte des métriques système
          echo "🔍 Métriques système:" > performance-metrics-${{ matrix.test-suite }}.md
          echo "- CPU: $(top -l 1 | grep 'CPU usage' | awk '{print \$3}')" >> performance-metrics-${{ matrix.test-suite }}.md
          echo "- Mémoire: $(vm_stat | grep 'Pages free' | awk '{print \$3}') pages libres" >> performance-metrics-${{ matrix.test-suite }}.md
          echo "- Disque: $(df -h / | awk 'NR==2 {print \$4}') libres" >> performance-metrics-${{ matrix.test-suite }}.md

          # Collecte des métriques Docker
          echo "" >> performance-metrics-${{ matrix.test-suite }}.md
          echo "🐳 Métriques Docker:" >> performance-metrics-${{ matrix.test-suite }}.md
          docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" >> performance-metrics-${{ matrix.test-suite }}.md

      - name: 📊 Run load tests with Locust
        run: |
          echo "📊 Tests de charge avec Locust..."

                    # Création d'un script Locust simple si nécessaire
          if [ ! -f "locustfile.py" ]; then
            echo 'from locust import HttpUser, task, between' > locustfile.py
            echo '' >> locustfile.py
            echo 'class ArkaliaUser(HttpUser):' >> locustfile.py
            echo '    wait_time = between(1, 3)' >> locustfile.py
            echo '' >> locustfile.py
            echo '    @task(3)' >> locustfile.py
            echo '    def health_check(self):' >> locustfile.py
            echo '        self.client.get("/health")' >> locustfile.py
            echo '' >> locustfile.py
            echo '    @task(2)' >> locustfile.py
            echo '    def zeroia_decision(self):' >> locustfile.py
            echo '        self.client.get("/zeroia/decision")' >> locustfile.py
            echo '' >> locustfile.py
            echo '    @task(1)' >> locustfile.py
            echo '    def reflexia_health(self):' >> locustfile.py
            echo '        self.client.get("/reflexia/health")' >> locustfile.py
          fi

          # Exécution des tests de charge
          locust -f locustfile.py --headless --users 10 --spawn-rate 2 --run-time 60s --html=load-test-report-${{ matrix.test-suite }}.html || echo "⚠️ Tests de charge terminés avec avertissements"

      - name: 📋 Upload performance artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ matrix.test-suite }}
          path: |
            .benchmarks/
            performance-metrics-${{ matrix.test-suite }}.md
            load-test-report-${{ matrix.test-suite }}.html
            performance-results/
          retention-days: 30

      - name: 🐳 Stop services
        if: always()
        run: |
          echo "🛑 Arrêt des services..."
          docker compose down --remove-orphans
          docker system prune -f

  # 📊 Analyse des performances
  performance-analysis:
    name: 📊 Analyse des Performances
    runs-on: ubuntu-latest
    needs: performance-tests
    if: always()
    timeout-minutes: 15
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pandas matplotlib

      - name: 📋 Download performance artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-artifacts/

      - name: 📊 Generate performance report
        run: |
          echo "## 🚀 Rapport Tests de Performance Arkalia-LUNA" > performance-report.md
          echo "### 📅 Date: $(date)" >> performance-report.md
          echo "### 🔗 Commit: ${{ github.sha }}" >> performance-report.md
          echo "### 🌿 Branche: ${{ github.ref }}" >> performance-report.md
          echo "### 🏃‍♂️ Event: ${{ github.event_name }}" >> performance-report.md
          echo "" >> performance-report.md
          echo "### ✅ Statut des Tests:" >> performance-report.md
          echo "- ZeroIA: ${{ needs.performance-tests.result }}" >> performance-report.md
          echo "- API: ${{ needs.performance-tests.result }}" >> performance-report.md
          echo "- Intégration: ${{ needs.performance-tests.result }}" >> performance-report.md
          echo "" >> performance-report.md
          echo "### 📊 Métriques Collectées:" >> performance-report.md

          # Analyse des résultats
          if [ -d "performance-artifacts" ]; then
            echo "📁 Artifacts disponibles:" >> performance-report.md
            find performance-artifacts/ -type f -name "*.md" -exec echo "- {}" \; >> performance-report.md
            find performance-artifacts/ -type f -name "*.html" -exec echo "- {}" \; >> performance-report.md
          else
            echo "⚠️ Aucun artifact trouvé" >> performance-report.md
          fi

          echo "" >> performance-report.md
          echo "### 🎯 Seuils de Performance:" >> performance-report.md
          echo "- Temps de réponse API: < 500ms" >> performance-report.md
          echo "- Décision ZeroIA: < 2s" >> performance-report.md
          echo "- Circuit breaker: < 10ms" >> performance-report.md
          echo "- Charge concurrente: 10 utilisateurs" >> performance-report.md

      - name: 📊 Run performance benchmark script
        run: |
          echo "📊 Exécution du script de benchmark..."
          if [ -f "scripts/ark-performance-benchmark.py" ]; then
            python scripts/ark-performance-benchmark.py --report-only --output-format=markdown >> performance-report.md
          else
            echo "⚠️ Script de benchmark non trouvé" >> performance-report.md
          fi

      - name: 📋 Upload performance analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis
          path: performance-report.md
          retention-days: 90

      - name: 🚨 Alert on performance regression
        if: failure()
        run: |
          echo "⚠️ Régression de performance détectée!"
          echo "🔗 Voir les détails: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Ici vous pourriez ajouter des alertes Slack/email
          # curl -X POST -H 'Content-type: application/json' --data '{"text":"⚠️ Régression de performance détectée!"}' ${{ secrets.SLACK_WEBHOOK }}

  # 🔄 Tests de performance de régression
  regression-tests:
    name: 🔄 Tests de Régression Performance
    runs-on: ubuntu-latest
    needs: performance-analysis
    if: github.event_name == 'schedule'
    timeout-minutes: 20
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🐍 Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: 📦 Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pytest pytest-benchmark

      - name: 🔄 Run regression tests
        run: |
          echo "🔄 Tests de régression de performance..."

          # Tests de régression basiques
          pytest tests/performance/ -v --benchmark-compare --benchmark-compare-fail=mean:10% || echo "⚠️ Tests de régression terminés avec avertissements"

      - name: 📊 Generate regression report
        run: |
          echo "## 🔄 Rapport Tests de Régression Performance" > regression-report.md
          echo "### 📅 Date: $(date)" >> regression-report.md
          echo "### 🔗 Commit: ${{ github.sha }}" >> regression-report.md
          echo "" >> regression-report.md
          echo "### 📊 Comparaison avec baseline:" >> regression-report.md
          echo "- Seuil de tolérance: 10%" >> regression-report.md
          echo "- Métriques comparées: temps de réponse, débit" >> regression-report.md

      - name: 📋 Upload regression report
        uses: actions/upload-artifact@v4
        with:
          name: regression-report
          path: regression-report.md
          retention-days: 90
