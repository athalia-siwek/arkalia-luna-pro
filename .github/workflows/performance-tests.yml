---
name: ğŸš€ Tests de Performance

on:
  push:
    branches: [ main, develop, dev-migration, refonte-stable ]
  pull_request:
    branches: [ main, develop, dev-migration, refonte-stable ]
  schedule:
    # Tests de performance quotidiens Ã  2h du matin
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: "3.10"
  PERFORMANCE_TIMEOUT: 1800  # 30 minutes

# Permissions pour Ã©viter les erreurs de sÃ©curitÃ©
permissions:
  contents: read
  actions: read

jobs:
  # ğŸš€ Tests de performance complets
  performance-tests:
    name: ğŸ“Š Tests de Performance Complets
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        python-version: ["3.10"]
        test-suite: ["zeroia", "api", "integration"]
      fail-fast: false
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-benchmark pytest-timeout
          pip install aiohttp requests locust

      - name: ğŸ§¹ Clean performance artifacts
        run: |
          find . -name "._*" -delete
          find . -name ".DS_Store" -delete
          find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
          rm -rf .benchmarks/ performance-results/

      - name: ğŸ³ Setup Docker for performance tests
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            image=moby/buildkit:v0.12.0

      - name: ğŸ³ Start services for performance testing
        run: |
          echo "ğŸš€ DÃ©marrage des services pour tests de performance..."
          echo "ğŸ”¨ Construction des images Docker..."
          docker compose build --no-cache
          docker compose up -d --remove-orphans
          sleep 60

      - name: ğŸ¥ Health check before performance tests
        run: |
          echo "ğŸ¥ VÃ©rification santÃ© avant tests de performance..."
          for service in "8000/health" "8000/zeroia/health" "8000/reflexia/health"; do
            echo "ğŸ” VÃ©rification $service..."
            for i in {1..10}; do
              if curl -f -s "http://localhost:$service" > /dev/null; then
                echo "âœ… $service disponible"
                break
              else
                echo "â³ Tentative $i/10 - $service non disponible"
                sleep 10
              fi
            done
          done

      - name: ğŸ§ª Run Performance Tests - ${{ matrix.test-suite }}
        run: |
          echo "ğŸ§ª ExÃ©cution des tests de performance - ${{ matrix.test-suite }}..."
          case "${{ matrix.test-suite }}" in
            "zeroia")
              echo "ğŸ“Š Tests de performance ZeroIA..."
              pytest tests/performance/zeroia/test_zeroia_performance.py::test_zeroia_decision_time_under_2s -v --benchmark-only
              pytest tests/performance/zeroia/test_zeroia_performance.py::test_circuit_breaker_latency_under_10ms -v --benchmark-only
              pytest tests/performance/zeroia/test_zeroia_performance.py::test_performance_regression_detection -v --benchmark-only
              ;;
            "api")
              echo "ğŸ“Š Tests de performance API..."
              pytest tests/performance/api/ -v --benchmark-only --timeout=300 || echo "âš ï¸ Tests API terminÃ©s avec avertissements"
              ;;
            "integration")
              echo "ğŸ“Š Tests de performance intÃ©gration..."
              pytest tests/performance/integration/ -v --benchmark-only --timeout=600 || echo "âš ï¸ Tests intÃ©gration terminÃ©s avec avertissements"
              ;;
          esac

      - name: ğŸ“Š Generate performance metrics
        run: |
          echo "ğŸ“Š GÃ©nÃ©ration des mÃ©triques de performance..."
          {
            echo "ğŸ” MÃ©triques systÃ¨me:";
            echo "- CPU: $(top -bn1 | grep 'Cpu(s)' | awk '{print \$2}')";
            echo "- MÃ©moire: $(free -h | grep 'Mem:' | awk '{print \$7}') libres";
            echo "- Disque: $(df -h / | awk 'NR==2 {print \$4}') libres";
            echo "";
            echo "ğŸ³ MÃ©triques Docker:";
            docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}";
          } > performance-metrics-${{ matrix.test-suite }}.md

      - name: ğŸ“Š Run load tests with Locust
        run: |
          echo "ğŸ“Š Tests de charge avec Locust..."
          if [ ! -f "locustfile.py" ]; then
            {
              echo 'from locust import HttpUser, task, between';
              echo '';
              echo 'class ArkaliaUser(HttpUser):';
              echo '    wait_time = between(1, 3)';
              echo '';
              echo '    @task(3)';
              echo '    def health_check(self):';
              echo '        self.client.get("/health")';
              echo '';
              echo '    @task(2)';
              echo '    def zeroia_decision(self):';
              echo '        self.client.get("/zeroia/decision")';
              echo '';
              echo '    @task(1)';
              echo '    def reflexia_health(self):';
              echo '        self.client.get("/reflexia/health")';
            } > locustfile.py
          fi
          locust -f locustfile.py --headless --users 10 --spawn-rate 2 --run-time 60s --html=load-test-report-${{ matrix.test-suite }}.html || echo "âš ï¸ Tests de charge terminÃ©s avec avertissements"

      - name: ğŸ“‹ Upload performance artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ matrix.test-suite }}
          path: |
            .benchmarks/
            performance-metrics-${{ matrix.test-suite }}.md
            load-test-report-${{ matrix.test-suite }}.html
            performance-results/
          retention-days: 30

      - name: ğŸ³ Stop services
        if: always()
        run: |
          echo "ğŸ›‘ ArrÃªt des services..."
          docker compose down --remove-orphans
          docker system prune -f

  # ğŸ“Š Analyse des performances
  performance-analysis:
    name: ğŸ“Š Analyse des Performances
    runs-on: ubuntu-latest
    needs: performance-tests
    if: always()
    timeout-minutes: 15
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pandas matplotlib

      - name: ğŸ“‹ Download performance artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-artifacts/

      - name: ğŸ“Š Generate performance report
        run: |
          {
            echo "## ğŸš€ Rapport Tests de Performance Arkalia-LUNA";
            echo "### ğŸ“… Date: $(date)";
            echo "### ğŸ”— Commit: ${{ github.sha }}";
            echo "### ğŸŒ¿ Branche: ${{ github.ref }}";
            echo "### ğŸƒâ€â™‚ï¸ Event: ${{ github.event_name }}";
            echo "";
            echo "### âœ… Statut des Tests:";
            echo "- ZeroIA: ${{ needs.performance-tests.result }}";
            echo "- API: ${{ needs.performance-tests.result }}";
            echo "- IntÃ©gration: ${{ needs.performance-tests.result }}";
            echo "";
            echo "### ğŸ“Š MÃ©triques CollectÃ©es:";
            if [ -d "performance-artifacts" ]; then
              echo "ğŸ“ Artifacts disponibles:";
              find performance-artifacts/ -type f -name "*.md" -exec echo "- {}" \;;
              find performance-artifacts/ -type f -name "*.html" -exec echo "- {}" \;;
            else
              echo "âš ï¸ Aucun artifact trouvÃ©";
            fi
            echo "";
            echo "### ğŸ¯ Seuils de Performance:";
            echo "- Temps de rÃ©ponse API: < 500ms";
            echo "- DÃ©cision ZeroIA: < 2s";
            echo "- Circuit breaker: < 10ms";
            echo "- Charge concurrente: 10 utilisateurs";
          } > performance-report.md

      - name: ğŸ“Š Run performance benchmark script
        run: |
          echo "ğŸ“Š ExÃ©cution du script de benchmark..."
          if [ -f "scripts/ark-performance-benchmark.py" ]; then
            python scripts/ark-performance-benchmark.py --report-only >> performance-report.md
          else
            echo "âš ï¸ Script de benchmark non trouvÃ©" >> performance-report.md
          fi

      - name: ğŸ“‹ Upload performance analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis
          path: performance-report.md
          retention-days: 90

      - name: ğŸš¨ Alert on performance regression
        if: failure()
        run: |
          echo "âš ï¸ RÃ©gression de performance dÃ©tectÃ©e!"
          echo "ğŸ”— Voir les dÃ©tails: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Ici vous pourriez ajouter des alertes Slack/email
          # curl -X POST -H 'Content-type: application/json' --data '{"text":"âš ï¸ RÃ©gression de performance dÃ©tectÃ©e!"}' ${{ secrets.SLACK_WEBHOOK }}

  # ğŸ”„ Tests de performance de rÃ©gression
  regression-tests:
    name: ğŸ”„ Tests de RÃ©gression Performance
    runs-on: ubuntu-latest
    needs: performance-analysis
    if: github.event_name == 'schedule'
    timeout-minutes: 20
    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pytest pytest-benchmark

      - name: ğŸ”„ Run regression tests
        run: |
          echo "ğŸ”„ Tests de rÃ©gression de performance..."
          pytest tests/performance/ -v --benchmark-compare --benchmark-compare-fail=mean:10% || echo "âš ï¸ Tests de rÃ©gression terminÃ©s avec avertissements"

      - name: ğŸ“Š Generate regression report
        run: |
          {
            echo "## ğŸ”„ Rapport Tests de RÃ©gression Performance";
            echo "### ğŸ“… Date: $(date)";
            echo "### ğŸ”— Commit: ${{ github.sha }}";
            echo "";
            echo "### ğŸ“Š Comparaison avec baseline:";
            echo "- Seuil de tolÃ©rance: 10%";
            echo "- MÃ©triques comparÃ©es: temps de rÃ©ponse, dÃ©bit";
          } > regression-report.md

      - name: ğŸ“‹ Upload regression report
        uses: actions/upload-artifact@v4
        with:
          name: regression-report
          path: regression-report.md
          retention-days: 90
