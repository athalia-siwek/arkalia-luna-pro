#!/usr/bin/env python3
"""
ðŸ§ª [MODEL POISONING TEST] - Roadmap S2 Arkalia-LUNA
Tests avancÃ©s de dÃ©tection d'empoisonnement de modÃ¨le IA

Tests 5 types d'attaques:
- CPU Injection, Oscillation, YAML Injection, Stealth, Normal
"""

import json
import os
import sys

# Configuration du path AVANT tout import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "modules"))

import tempfile
from datetime import datetime
from pathlib import Path

import toml

from modules.zeroia.model_integrity import get_integrity_monitor
from modules.zeroia.reason_loop import reason_loop
from tests.security.test_poisoning import FakePoisonedDatasets, ModelPoisoningDetector


def test_live_poisoning_attacks():
    """Test en conditions rÃ©elles avec fichiers temporaires"""
    print("ðŸ§ª [POISONING TEST] DÃ©marrage tests model poisoning...")

    fake_data = FakePoisonedDatasets()
    monitor = get_integrity_monitor()

    # RÃ©sultats de test
    test_results = {
        "cpu_injection": {"status": "PENDING", "details": ""},
        "oscillation_attack": {"status": "PENDING", "details": ""},
        "yaml_injection": {"status": "PENDING", "details": ""},
        "stealth_poisoning": {"status": "PENDING", "details": ""},
        "normal_operation": {"status": "PENDING", "details": ""},
    }

    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        # Test 1: CPU Injection Attack
        print("\nðŸ”¥ Test 1: CPU Injection Attack")
        try:
            poisoned_ctx = fake_data.create_cpu_injection_attack()
            ctx_file = tmp_path / "ctx_poison.toml"
            state_file = tmp_path / "state_poison.toml"
            reflexia_file = tmp_path / "reflexia_poison.toml"

            # Sauvegarde contexte empoisonnÃ©
            with open(ctx_file, "w") as f:
                toml.dump(poisoned_ctx, f)

            # Reflexia state normal
            reflexia_state = {"decision": {"last_decision": "normal", "confidence": 0.8}}
            with open(reflexia_file, "w") as f:
                toml.dump(reflexia_state, f)

            # ExÃ©cution avec validation intÃ©gritÃ©
            decision, confidence = reason_loop(
                context_path=ctx_file,
                reflexia_path=reflexia_file,
                state_path=state_file,
            )

            integrity_status = monitor.get_integrity_status()

            if integrity_status["status"] in ["SUSPICIOUS", "COMPROMISED"]:
                test_results["cpu_injection"]["status"] = "PROTECTED"
                test_results["cpu_injection"][
                    "details"
                ] = f"Attaque dÃ©tectÃ©e - Status: {integrity_status['status']}"
                print(f"âœ… Attaque CPU injection DÃ‰TECTÃ‰E - {integrity_status['status']}")
            else:
                test_results["cpu_injection"]["status"] = "VULNERABLE"
                test_results["cpu_injection"]["details"] = "Attaque non dÃ©tectÃ©e"
                print("âŒ Attaque CPU injection NON DÃ‰TECTÃ‰E")

        except Exception as e:
            test_results["cpu_injection"]["status"] = "ERROR"
            test_results["cpu_injection"]["details"] = str(e)
            print(f"âš ï¸ Erreur test CPU injection: {e}")

        # Test 2: Oscillation Attack
        print("\nðŸŒ€ Test 2: Oscillation Attack")
        try:
            contexts = fake_data.create_oscillation_attack()
            decisions = []

            for i, ctx in enumerate(contexts[:5]):  # Limiter Ã  5 pour le test
                ctx_file = tmp_path / f"ctx_osc_{i}.toml"
                state_file = tmp_path / f"state_osc_{i}.toml"
                reflexia_file = tmp_path / f"reflexia_osc_{i}.toml"

                with open(ctx_file, "w") as f:
                    toml.dump(ctx, f)

                reflexia_state = {"decision": {"last_decision": "monitor", "confidence": 0.7}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )
                decisions.append(decision)

            # Analyse pattern oscillation
            detector = ModelPoisoningDetector()
            analysis = detector.analyze_decision_pattern(decisions)

            if analysis["anomaly_detected"]:
                test_results["oscillation_attack"]["status"] = "PROTECTED"
                test_results["oscillation_attack"][
                    "details"
                ] = f"Oscillation dÃ©tectÃ©e - Confidence: {analysis['confidence']:.2f}"
                confidence_val = analysis["confidence"]
                print(f"âœ… Attaque oscillation DÃ‰TECTÃ‰E - Confidence: {confidence_val:.2f}")
            else:
                test_results["oscillation_attack"]["status"] = "VULNERABLE"
                test_results["oscillation_attack"]["details"] = "Oscillation non dÃ©tectÃ©e"
                print("âŒ Attaque oscillation NON DÃ‰TECTÃ‰E")

        except Exception as e:
            test_results["oscillation_attack"]["status"] = "ERROR"
            test_results["oscillation_attack"]["details"] = str(e)
            print(f"âš ï¸ Erreur test oscillation: {e}")

        # Test 3: YAML Injection
        print("\nðŸ’‰ Test 3: YAML Injection Attack")
        try:
            malicious_ctx = fake_data.create_yaml_injection_attack()
            ctx_file = tmp_path / "ctx_yaml.toml"

            # Test si l'injection cause erreur (comportement attendu)
            try:
                with open(ctx_file, "w") as f:
                    toml.dump(malicious_ctx, f)

                reflexia_file = tmp_path / "reflexia_yaml.toml"
                state_file = tmp_path / "state_yaml.toml"

                reflexia_state = {"decision": {"last_decision": "normal", "confidence": 0.8}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )

                # Si on arrive ici, ZeroIA a rÃ©sistÃ©
                test_results["yaml_injection"]["status"] = "PROTECTED"
                test_results["yaml_injection"]["details"] = "Injection gÃ©rÃ©e gracieusement"
                print(f"âœ… Injection YAML gÃ©rÃ©e - Decision: {decision}")

            except (ValueError, TypeError, toml.TomlDecodeError) as validation_error:
                # Erreur attendue = protection effective
                test_results["yaml_injection"]["status"] = "PROTECTED"
                test_results["yaml_injection"]["details"] = f"Injection bloquÃ©e: {validation_error}"
                print(f"âœ… Injection YAML BLOQUÃ‰E: {validation_error}")

        except Exception as e:
            test_results["yaml_injection"]["status"] = "ERROR"
            test_results["yaml_injection"]["details"] = str(e)
            print(f"âš ï¸ Erreur test YAML injection: {e}")

        # Test 4: Stealth Poisoning
        print("\nðŸ¥· Test 4: Stealth Poisoning")
        try:
            stealth_ctx = fake_data.create_stealth_poisoning()
            decisions = []

            # RÃ©pÃ©ter 6 fois le mÃªme contexte "furtif"
            for i in range(6):
                ctx_file = tmp_path / f"ctx_stealth_{i}.toml"
                state_file = tmp_path / f"state_stealth_{i}.toml"
                reflexia_file = tmp_path / f"reflexia_stealth_{i}.toml"

                with open(ctx_file, "w") as f:
                    toml.dump(stealth_ctx, f)

                reflexia_state = {"decision": {"last_decision": "reduce_load", "confidence": 0.79}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )
                decisions.append(decision)

            # Analyse rÃ©pÃ©tition suspecte
            if all(d == decisions[0] for d in decisions):
                integrity_status = monitor.get_integrity_status()
                if integrity_status["anomalies_detected"] > 0:
                    test_results["stealth_poisoning"]["status"] = "PROTECTED"
                    test_results["stealth_poisoning"]["details"] = (
                        f"RÃ©pÃ©tition suspecte dÃ©tectÃ©e - "
                        f"Anomalies: {integrity_status['anomalies_detected']}"
                    )
                    print("âœ… Empoisonnement furtif DÃ‰TECTÃ‰")
                else:
                    test_results["stealth_poisoning"]["status"] = "VULNERABLE"
                    test_results["stealth_poisoning"][
                        "details"
                    ] = "RÃ©pÃ©tition suspecte non dÃ©tectÃ©e"
                    print("âŒ Empoisonnement furtif NON DÃ‰TECTÃ‰")
            else:
                test_results["stealth_poisoning"]["status"] = "INDETERMINATE"
                test_results["stealth_poisoning"][
                    "details"
                ] = "DÃ©cisions variÃ©es - test non concluant"
                print("ðŸ¤” Test furtif INDÃ‰TERMINÃ‰")

        except Exception as e:
            test_results["stealth_poisoning"]["status"] = "ERROR"
            test_results["stealth_poisoning"]["details"] = str(e)
            print(f"âš ï¸ Erreur test stealth: {e}")

        # Test 5: OpÃ©ration normale (baseline)
        print("\nâœ… Test 5: Normal Operation Baseline")
        try:
            normal_contexts = [
                {"status": {"cpu": 45, "severity": "none", "ram": 40, "disk": 25}},
                {"status": {"cpu": 55, "severity": "none", "ram": 45, "disk": 30}},
                {"status": {"cpu": 62, "severity": "none", "ram": 50, "disk": 35}},
            ]

            decisions = []
            for i, ctx in enumerate(normal_contexts):
                ctx_file = tmp_path / f"ctx_normal_{i}.toml"
                state_file = tmp_path / f"state_normal_{i}.toml"
                reflexia_file = tmp_path / f"reflexia_normal_{i}.toml"

                with open(ctx_file, "w") as f:
                    toml.dump(ctx, f)

                reflexia_state = {"decision": {"last_decision": "monitor", "confidence": 0.7}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )
                decisions.append(decision)

            final_status = monitor.get_integrity_status()
            if final_status["status"] == "HEALTHY":
                test_results["normal_operation"]["status"] = "PASS"
                test_results["normal_operation"]["details"] = "OpÃ©ration normale non perturbÃ©e"
                print("âœ… OpÃ©ration normale PRÃ‰SERVÃ‰E")
            else:
                test_results["normal_operation"]["status"] = "FAIL"
                test_results["normal_operation"][
                    "details"
                ] = f"Faux positifs dÃ©tectÃ©s - Status: {final_status['status']}"
                print("âš ï¸ Faux positifs dÃ©tectÃ©s")

        except Exception as e:
            test_results["normal_operation"]["status"] = "ERROR"
            test_results["normal_operation"]["details"] = str(e)
            print(f"âš ï¸ Erreur test normal: {e}")

    # Rapport final
    print(f"\n{'='*60}")
    print("ðŸ›¡ï¸ RAPPORT FINAL - MODEL POISONING DETECTION")
    print(f"{'='*60}")

    protected_count = sum(
        1 for result in test_results.values() if result["status"] in ["PROTECTED", "PASS"]
    )

    for test_name, result in test_results.items():
        status_emoji = {
            "PROTECTED": "ðŸ›¡ï¸",
            "PASS": "âœ…",
            "VULNERABLE": "âŒ",
            "ERROR": "âš ï¸",
            "INDETERMINATE": "ðŸ¤”",
        }.get(result["status"], "â“")

        print(f"{status_emoji} {test_name.upper()}: {result['status']}")
        print(f"   â””â”€ {result['details']}")

    protection_rate = (protected_count / len(test_results)) * 100
    total_tests = len(test_results)
    print(f"\nðŸ“Š TAUX DE PROTECTION: {protection_rate:.1f}% " f"({protected_count}/{total_tests})")

    if protection_rate >= 80:
        print("ðŸŽ‰ EXCELLENT: ZeroIA rÃ©siste aux attaques d'empoisonnement !")
    elif protection_rate >= 60:
        print("ðŸ‘ BON: Protection solide, quelques amÃ©liorations possibles")
    else:
        print("âš ï¸ ATTENTION: VulnÃ©rabilitÃ©s dÃ©tectÃ©es - renforcement nÃ©cessaire")

    # Sauvegarde rapport
    report_file = Path("logs/model_poisoning_test_report.json")
    report_file.parent.mkdir(exist_ok=True)

    full_report = {
        "timestamp": datetime.now().isoformat(),
        "protection_rate": protection_rate,
        "test_results": test_results,
        "integrity_final_status": monitor.get_integrity_status(),
    }

    with open(report_file, "w") as f:
        json.dump(full_report, f, indent=2)

    print(f"\nðŸ“„ Rapport sauvegardÃ©: {report_file}")

    return protection_rate >= 60  # Retourne True si protection acceptable


if __name__ == "__main__":
    success = test_live_poisoning_attacks()
    sys.exit(0 if success else 1)
