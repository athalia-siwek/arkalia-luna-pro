#!/usr/bin/env python3
"""
üß™ [MODEL POISONING TEST] - Roadmap S2 Arkalia-LUNA
Tests avanc√©s de d√©tection d'empoisonnement de mod√®le IA

Tests 5 types d'attaques:
- CPU Injection, Oscillation, YAML Injection, Stealth, Normal
"""

import json
import os
import sys

# Configuration du path AVANT tout import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "modules"))

import tempfile
from datetime import datetime
from pathlib import Path

import toml

from modules.zeroia.model_integrity import get_integrity_monitor
from modules.zeroia.reason_loop import reason_loop
from tests.security.test_poisoning import FakePoisonedDatasets, ModelPoisoningDetector


def test_live_poisoning_attacks():
    """Test en conditions r√©elles avec fichiers temporaires"""
    print("üß™ [POISONING TEST] D√©marrage tests model poisoning...")

    fake_data = FakePoisonedDatasets()
    monitor = get_integrity_monitor()

    # R√©sultats de test
    test_results = {
        "cpu_injection": {"status": "PENDING", "details": ""},
        "oscillation_attack": {"status": "PENDING", "details": ""},
        "yaml_injection": {"status": "PENDING", "details": ""},
        "stealth_poisoning": {"status": "PENDING", "details": ""},
        "normal_operation": {"status": "PENDING", "details": ""},
    }

    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        # Test 1: CPU Injection Attack
        print("\nüî• Test 1: CPU Injection Attack")
        try:
            poisoned_ctx = fake_data.create_cpu_injection_attack()
            ctx_file = tmp_path / "ctx_poison.toml"
            state_file = tmp_path / "state_poison.toml"
            reflexia_file = tmp_path / "reflexia_poison.toml"

            # Sauvegarde contexte empoisonn√©
            with open(ctx_file, "w") as f:
                toml.dump(poisoned_ctx, f)

            # Reflexia state normal
            reflexia_state = {"decision": {"last_decision": "normal", "confidence": 0.8}}
            with open(reflexia_file, "w") as f:
                toml.dump(reflexia_state, f)

            # Ex√©cution avec validation int√©grit√©
            decision, confidence = reason_loop(
                context_path=ctx_file,
                reflexia_path=reflexia_file,
                state_path=state_file,
            )

            integrity_status = monitor.get_integrity_status()

            if integrity_status["status"] in ["SUSPICIOUS", "COMPROMISED"]:
                test_results["cpu_injection"]["status"] = "PROTECTED"
                test_results["cpu_injection"][
                    "details"
                ] = f"Attaque d√©tect√©e - Status: {integrity_status['status']}"
                print(f"‚úÖ Attaque CPU injection D√âTECT√âE - {integrity_status['status']}")
            else:
                test_results["cpu_injection"]["status"] = "VULNERABLE"
                test_results["cpu_injection"]["details"] = "Attaque non d√©tect√©e"
                print("‚ùå Attaque CPU injection NON D√âTECT√âE")

        except Exception as e:
            test_results["cpu_injection"]["status"] = "ERROR"
            test_results["cpu_injection"]["details"] = str(e)
            print(f"‚ö†Ô∏è Erreur test CPU injection: {e}")

        # Test 2: Oscillation Attack
        print("\nüåÄ Test 2: Oscillation Attack")
        try:
            contexts = fake_data.create_oscillation_attack()
            decisions = []

            for i, ctx in enumerate(contexts[:5]):  # Limiter √† 5 pour le test
                ctx_file = tmp_path / f"ctx_osc_{i}.toml"
                state_file = tmp_path / f"state_osc_{i}.toml"
                reflexia_file = tmp_path / f"reflexia_osc_{i}.toml"

                with open(ctx_file, "w") as f:
                    toml.dump(ctx, f)

                reflexia_state = {"decision": {"last_decision": "monitor", "confidence": 0.7}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )
                decisions.append(decision)

            # Analyse pattern oscillation
            detector = ModelPoisoningDetector()
            analysis = detector.analyze_decision_pattern(decisions)

            if analysis["anomaly_detected"]:
                test_results["oscillation_attack"]["status"] = "PROTECTED"
                test_results["oscillation_attack"][
                    "details"
                ] = f"Oscillation d√©tect√©e - Confidence: {analysis['confidence']:.2f}"
                confidence_val = analysis["confidence"]
                print(f"‚úÖ Attaque oscillation D√âTECT√âE - Confidence: {confidence_val:.2f}")
            else:
                test_results["oscillation_attack"]["status"] = "VULNERABLE"
                test_results["oscillation_attack"]["details"] = "Oscillation non d√©tect√©e"
                print("‚ùå Attaque oscillation NON D√âTECT√âE")

        except Exception as e:
            test_results["oscillation_attack"]["status"] = "ERROR"
            test_results["oscillation_attack"]["details"] = str(e)
            print(f"‚ö†Ô∏è Erreur test oscillation: {e}")

        # Test 3: YAML Injection
        print("\nüíâ Test 3: YAML Injection Attack")
        try:
            malicious_ctx = fake_data.create_yaml_injection_attack()
            ctx_file = tmp_path / "ctx_yaml.toml"

            # Test si l'injection cause erreur (comportement attendu)
            try:
                with open(ctx_file, "w") as f:
                    toml.dump(malicious_ctx, f)

                reflexia_file = tmp_path / "reflexia_yaml.toml"
                state_file = tmp_path / "state_yaml.toml"

                reflexia_state = {"decision": {"last_decision": "normal", "confidence": 0.8}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )

                # Si on arrive ici, ZeroIA a r√©sist√©
                test_results["yaml_injection"]["status"] = "PROTECTED"
                test_results["yaml_injection"]["details"] = "Injection g√©r√©e gracieusement"
                print(f"‚úÖ Injection YAML g√©r√©e - Decision: {decision}")

            except (ValueError, TypeError, toml.TomlDecodeError) as validation_error:
                # Erreur attendue = protection effective
                test_results["yaml_injection"]["status"] = "PROTECTED"
                test_results["yaml_injection"]["details"] = f"Injection bloqu√©e: {validation_error}"
                print(f"‚úÖ Injection YAML BLOQU√âE: {validation_error}")

        except Exception as e:
            test_results["yaml_injection"]["status"] = "ERROR"
            test_results["yaml_injection"]["details"] = str(e)
            print(f"‚ö†Ô∏è Erreur test YAML injection: {e}")

        # Test 4: Stealth Poisoning
        print("\nü•∑ Test 4: Stealth Poisoning")
        try:
            stealth_ctx = fake_data.create_stealth_poisoning()
            decisions = []

            # R√©p√©ter 6 fois le m√™me contexte "furtif"
            for i in range(6):
                ctx_file = tmp_path / f"ctx_stealth_{i}.toml"
                state_file = tmp_path / f"state_stealth_{i}.toml"
                reflexia_file = tmp_path / f"reflexia_stealth_{i}.toml"

                with open(ctx_file, "w") as f:
                    toml.dump(stealth_ctx, f)

                reflexia_state = {"decision": {"last_decision": "reduce_load", "confidence": 0.79}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )
                decisions.append(decision)

            # Analyse r√©p√©tition suspecte
            if all(d == decisions[0] for d in decisions):
                integrity_status = monitor.get_integrity_status()
                if integrity_status["anomalies_detected"] > 0:
                    test_results["stealth_poisoning"]["status"] = "PROTECTED"
                    test_results["stealth_poisoning"]["details"] = (
                        f"R√©p√©tition suspecte d√©tect√©e - "
                        f"Anomalies: {integrity_status['anomalies_detected']}"
                    )
                    print("‚úÖ Empoisonnement furtif D√âTECT√â")
                else:
                    test_results["stealth_poisoning"]["status"] = "VULNERABLE"
                    test_results["stealth_poisoning"][
                        "details"
                    ] = "R√©p√©tition suspecte non d√©tect√©e"
                    print("‚ùå Empoisonnement furtif NON D√âTECT√â")
            else:
                test_results["stealth_poisoning"]["status"] = "INDETERMINATE"
                test_results["stealth_poisoning"][
                    "details"
                ] = "D√©cisions vari√©es - test non concluant"
                print("ü§î Test furtif IND√âTERMIN√â")

        except Exception as e:
            test_results["stealth_poisoning"]["status"] = "ERROR"
            test_results["stealth_poisoning"]["details"] = str(e)
            print(f"‚ö†Ô∏è Erreur test stealth: {e}")

        # Test 5: Op√©ration normale (baseline)
        print("\n‚úÖ Test 5: Normal Operation Baseline")
        try:
            normal_contexts = [
                {"status": {"cpu": 45, "severity": "none", "ram": 40, "disk": 25}},
                {"status": {"cpu": 55, "severity": "none", "ram": 45, "disk": 30}},
                {"status": {"cpu": 62, "severity": "none", "ram": 50, "disk": 35}},
            ]

            decisions = []
            for i, ctx in enumerate(normal_contexts):
                ctx_file = tmp_path / f"ctx_normal_{i}.toml"
                state_file = tmp_path / f"state_normal_{i}.toml"
                reflexia_file = tmp_path / f"reflexia_normal_{i}.toml"

                with open(ctx_file, "w") as f:
                    toml.dump(ctx, f)

                reflexia_state = {"decision": {"last_decision": "monitor", "confidence": 0.7}}
                with open(reflexia_file, "w") as f:
                    toml.dump(reflexia_state, f)

                decision, confidence = reason_loop(
                    context_path=ctx_file,
                    reflexia_path=reflexia_file,
                    state_path=state_file,
                )
                decisions.append(decision)

            final_status = monitor.get_integrity_status()
            if final_status["status"] == "HEALTHY":
                test_results["normal_operation"]["status"] = "PASS"
                test_results["normal_operation"]["details"] = "Op√©ration normale non perturb√©e"
                print("‚úÖ Op√©ration normale PR√âSERV√âE")
            else:
                test_results["normal_operation"]["status"] = "FAIL"
                test_results["normal_operation"][
                    "details"
                ] = f"Faux positifs d√©tect√©s - Status: {final_status['status']}"
                print("‚ö†Ô∏è Faux positifs d√©tect√©s")

        except Exception as e:
            test_results["normal_operation"]["status"] = "ERROR"
            test_results["normal_operation"]["details"] = str(e)
            print(f"‚ö†Ô∏è Erreur test normal: {e}")

    # Rapport final
    print(f"\n{'='*60}")
    print("üõ°Ô∏è RAPPORT FINAL - MODEL POISONING DETECTION")
    print(f"{'='*60}")

    protected_count = sum(
        1 for result in test_results.values() if result["status"] in ["PROTECTED", "PASS"]
    )

    for test_name, result in test_results.items():
        status_emoji = {
            "PROTECTED": "üõ°Ô∏è",
            "PASS": "‚úÖ",
            "VULNERABLE": "‚ùå",
            "ERROR": "‚ö†Ô∏è",
            "INDETERMINATE": "ü§î",
        }.get(result["status"], "‚ùì")

        print(f"{status_emoji} {test_name.upper()}: {result['status']}")
        print(f"   ‚îî‚îÄ {result['details']}")

    protection_rate = (protected_count / len(test_results)) * 100
    total_tests = len(test_results)
    print(f"\nüìä TAUX DE PROTECTION: {protection_rate:.1f}% " f"({protected_count}/{total_tests})")

    if protection_rate >= 80:
        print("üéâ EXCELLENT: ZeroIA r√©siste aux attaques d'empoisonnement !")
    elif protection_rate >= 60:
        print("üëç BON: Protection solide, quelques am√©liorations possibles")
    else:
        print("‚ö†Ô∏è ATTENTION: Vuln√©rabilit√©s d√©tect√©es - renforcement n√©cessaire")

    # Sauvegarde rapport
    report_file = Path("logs/model_poisoning_test_report.json")
    report_file.parent.mkdir(exist_ok=True)

    full_report = {
        "timestamp": datetime.now().isoformat(),
        "protection_rate": protection_rate,
        "test_results": test_results,
        "integrity_final_status": monitor.get_integrity_status(),
    }

    with open(report_file, "w") as f:
        json.dump(full_report, f, indent=2)

    print(f"\nüìÑ Rapport sauvegard√©: {report_file}")

    return protection_rate >= 60  # Retourne True si protection acceptable


if __name__ == "__main__":
    success = test_live_poisoning_attacks()
    sys.exit(0 if success else 1)
